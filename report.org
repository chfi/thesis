# -*- eval: (let () (org-babel-goto-named-src-block "setup-export-class") (org-babel-execute-src-block)); -*-

#+TITLE: TODO thesis name
#+AUTHOR: Christian Fischer
#+EMAIL: christian@chfi.se

# Define custom org-latex-class for exporting with correct headers
#+NAME: setup-export-class
#+BEGIN_SRC emacs-lisp :exports none :results none
  (add-to-list 'org-latex-classes
               '("thesis-report"
                 "\\documentclass[11pt]{report}"
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
  ("\\paragraph{%s}" . "\\paragraph*{%s}")))
#+END_SRC

#+begin_comment
#+BEGIN_SRC emacs-lisp :exports none :results silent
  (add-to-list 'org-latex-classes
               '("thesis-report" "\\documentclass[11pt]{report}"
  ("\\part{%s}" . "\\part*{%s}")
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}"))
#+END_SRC
#+end_comment

#+BIND: org-export-filter-plain-text-functions (tmp-f-symbols)
#+BEGIN_SRC emacs-lisp :exports results :results none
(defun tmp-f-symbols (text backend info)
  (when (org-export-derived-backend-p backend 'latex)
        (replace-regexp-in-string "<>" "\Diamond" text)))
#+END_SRC


#+LATEX_CLASS: thesis-report
#+LATEX_CLASS_OPTIONS: [a4paper]


#+LATEX_HEADER: \usepackage[numbers]{natbib}
#+OPTIONS: H:6


* Introduction

# TODO Have this here or in Background/Theory?
#+BEGIN_QUOTE
Legacy code. The phrase strikes disgust in the hearts of programmers.
It conjures images of slogging through a murky swamp of tangled
undergrowth with leaches beneath and stinging flies above. It conjures
odors of murk, slime, stagnancy, and offal. Although our first joy of
programming may have been intense, the misery of dealing with legacy
code is often sufficient to extinguish that flame. --- Robert C.
Martin, Foreword to Working Effectively with Legacy Code
\cite{feathers2004working}
#+END_QUOTE

# TODO redo this probably; opinionated & without citations~~
Problems related to and caused by legacy code are ubiquitous. Maintenance
of legacy systems costs billions of US dollars every year, and as time
goes on, the number of legacy systems in production will continue growing,
as what is considered modern today is likely to become "legacy" tomorrow.
In fact, one does not need to go back far in time to find "legacy" code,
as any code that is difficult to understand, and it is difficult to make
changes to such that the effects of the changes on the system are predictable,
can be considered "legacy" code.

Many solutions have been proposed and tried, yet legacy code continues
to be a problem. In fact, it will likely continue to be a problem
--- not until a way to "repair" legacy code is found, but a way to
write code that does not /become/ legacy code is found\cite{weide1995reverse}.

# TODO add more on FP?
This thesis is concerned with evaluating purely functional programming
as one potential solution to and preventative measure against legacy
code problems. This was done by developing a web application in PureScript
that both embeds an existing JavaScript application, as well as provides
novel functionality. The process, and resulting code, were evaluated
# TODO probably expand & give brief definition of signs here!
by looking for signs of legacy code problems, as defined in the Method
chapter.


** Objective
# see if FP can solve legacy code by developing GGB

# TODO reword these
The objective of the thesis consists of evaluating purely functional
programming as a tool to:

1. work with and extend an existing legacy system;
2. develop a "legacy-resistent" application.

# TODO move Method/Extending a Legacy System/Biodalliance here?
The legacy system in question is Biodalliance, a genome browser written
in JavaScript.


# TODO briefly introduce GN2 and BD
# TODO introduce problems w/ BD
# TODO introduce GGB as solution to problems;
# TODO purpose: explore FP as solution to JS legacy garbo


** Delimitations

This thesis concerns a single programming project and a single programmer;
it is a case study more than anything else, and so cannot be taken as
definite evidence.
# TODO this: However, it is hoped that

** Background/Context/Motivation
# GN2, BD etc., GSoC

** GeneNetwork 2/Affiliations


* Background/Theory
The prerequisite theory for the thesis project is introduced. A definition
of "legacy code" is given, followed by relevant statistics, facts, and
techniques concerning legacy code and working with it. This is followed by an introduction
to the purely functional programming paradigm.



** Legacy code
# TODO subsections here could use different names
# Section abstract


*** Definition

# What legacy code is - definition
# Wikipedia defines "legacy system":
# "In computing, a legacy system is an old method, technology, computer system, or
# application program, 'of, relating to, or being a previous or outdated computer
# system.' [..] This can also imply that the system is out of date or in need of
# replacement."

There is no formal definition of "legacy code", but \cite{feathers2004working}
gives the definition "Legacy code is code that we've gotten from someone else."
\citeauthor{Bennett1995} provides a definition of legacy systems in general, which
gives an idea of why it may be problematic: "large software systems that we don't know how to cope with but that are vital to
our organization" \cite{Bennett1995}.
Finally, \citeauthor{weide1995reverse} gives a definition closer to the
spirit of the concept as experienced by programmers working with
legacy systems --- in the trenches, as it were:

#+BEGIN_QUOTE
[..] legacy code, i.e., programs in which too much has been invested
just to throw away but which have proved to be obscure, mysterious,
and brittle in the face of maintenance.
#+END_QUOTE

In other words, legacy code is code that continues to be relevant, e.g. by
providing a service that is important, and that requires modification,
or will require modification. If there were never going to be any reason to
modify the code, it would not be worth talking about, nor is it likely that
a system that provides a continually valuable service will not at some point
in the future require maintenance or new features \cite{lehman1980programs}.

# TODO could maybe do with a bit more
# WIP statistics on legacy systems; how pervasive are they, how old do they
     # tend to be, how expensive is their maintenance and where do most of
     # the costs fall

For this reason, legacy systems are prevalent in the world.
If a system works as it should, providing the service that is needed,
and said service must continue to be provided, the safest thing to do
is to leave it as is --- until it is decided, for whatever reason, that
changes must be made.

The U.S. goverment federal IT budget for 2017 was over $89 billion, with
nearly 60% budgeted for operations and maintenance of agency systems,
with a study by the U.S. Government Accountability Office finding that
some federal agencies use systems that are up to 50 years old \cite{gao2016legacy}.

# TODO Footnote The Danger of Legacy Systems https://web.archive.org/web/20120323165836/http://www.mousesecurity.com/?p=220


Many of these federal agency systems consist of old hardware, old operating
systems, etc., however the problems of a legacy system do not need to be
caused by such factors. The code itself is often the problem \cite{Bisbal1999},
and is what this thesis is concerned with.

# TODO maybe move this paragraph, and maybe the previous one, to a more suitable position
We define a "legacy codebase" as the codebase of a legacy system,
where the problem of modifying the system is constrained by the code
itself --- the underlying technology is not relevant. Likewise we do not look
at dependencies, a problem solved by pure functional
package managers such as Nix \cite{dolstra2004nix} and Guix \cite{courtes2013functional}.

Why would changes need to be made to a legacy codebase? When the
behavior of the system needs to be changed. \cite{feathers2004working}
identifies four general reasons:

# TODO Maybe add examples?
# - security fixes
# - bug fixes
# - changes to external dependencies
# - changed features
# - new features (e.g. electronic healthcare records)
  1. Adding a feature
  2. Fixing a bug
  3. Improving the design
  4. Optimizing resource usage

All of these somehow modify the behavior of the system; if there was
no change in the system behavior, the change in code must have been
to some part of the codebase that isn't used! Thus, the desired change
requires a change in behavior. The problem with legacy code is that
it is difficult to know how to make the change to the code that produces
this desired change in behavior, and /only/ the desired change.


# Rewrite & use these two paragraphs if more concrete examples are desired
# *maaaybeeee* include this
# WIP The existing data structures etc. can be arbitrarily complex,
#      have old/unused data/fields, be mutated at various places;
#      thus it is difficult to know where to start when inserting
#      new code
# If the system has grown organically over a longer period of time,
# the data structures and procedures that manipulate them have likely
# also grown to fit new features etc., leading to large pieces of
# state that are difficult to reason about. Objects may be doing
# something much different from their original purpose. WIP
# TODO REF

# also *maaaybeeee* include this
# WIP Some new feature/solution to a new problem may involve
#      ""stuff (architecture etc.)"" that the existing code
#      is difficult to fit into, or vice versa
# The changes that need to be made may fundamentally be "out of phase"
# with the existing system. For example, it may require data that
# does exist in some part of the system, but the data is tangled
# up with other state and so on.

# Why legacy code is a *difficult* problem; i.e. how people have tried (and failed) to solve it
# or, equivalently, why it is still a problem

# WIP Why is it difficult to do these things, to change legacy code?
#     Nobody understands the code => the set of changes that are safe
#     to make are unknown


The main reason it is difficult to work with legacy code is lack of
knowledge of the system and codebase, and how the system's behavior
relates to the underlying code. Legacy codebases often lack
documentation and tests, without which a new programmer on the project
has few, if any, tools at their disposal to understand the codebase as
it is, since they do not have any knowledge of how and why the code
came to be as it is. Even if a design or system specification exists,
it is not necessarily accurate. The code may very well have grown
beyond the initial specification, and the specification need not have
been updated in step with the code.

For these reasons, one of the main problems of working with legacy
code is understanding it in the first place \cite{feathers2004working}
\cite{Bennett1995} \cite{siebra2016anticipated}. This is also a
difficult, time-consuming process, and one of the reasons reverse
engineering legacy systems is rarely, if ever, a cost-effective
undertaking \cite{weide1995reverse}. Also according to \citeauthor*{weide1995reverse},
even if a system is successfully reverse engineered and modified,
even if a /new/ system is successfully developed that provides
the same behavior as the legacy system but with a better design,
it is highly likely that the new system, eventually, reaches
a point where it too must be reverse engineered --- i.e. when the
"new" system becomes another legacy system, and the cycle begins anew.
# TODO we can drop a reference to samsara & nirvana here somewhere

In short, the problem with legacy code is lack of knowledge in what
the system does, and how the code relates to the system and its parts.
This makes it difficult to know what changes to make to the code to
produce the desired change in system behavior, and if a change made is safe,
i.e.\ that /no/ undesired change in system behavior results.


*** Solutions

Legacy code has been a recognized problem for decades, and people have
been trying to solve it for as long.
\cite{lehman1980programs}
\cite{weide1995reverse}
\cite{Mntyl2006}
This section will present some general approaches and tools that have
been applied when working with legacy systems. First, techniques that
help with manual reverse-engineering --- the most widely used approach
to solve legacy code problems --- are introduced, followed by a brief
walk through some automated tools.

**** Reverse engineering legacy systems

Reverse engineering a system can be very difficult and time-consuming,
even with access to the source code \cite{weide1995reverse}. There are
entire books dedicated to the subject of understanding a legacy codebase
and working with it to transform it into something more easily understood
and extended \cite{feathers2004working}.

As previously mentioned, the
greatest problems stem from insufficient knowledge of the system; from
not knowing what code does, or what happens when a change is made.
Thus, adding tests is one of the main tools for increasing understanding,
as tests provide the programmer with feedback on their code changes.
\cite{siebra2016anticipated}
However, this feedback is only as good as the test suite; if a
system behavior is not covered by the tests, the programmer has a blind
spot in that area.
Where and how to add tests is an art and science of its own, as the
programmer must find what parts of the system need to be tested, and
how to insert the tests into the codebase. This becomes more difficult
when a program has many tightly-knit parts, global state, etc.
# TODO REFs

With a robust test suite, a programmer can modify the codebase to improve the code quality
and architecture of the system, confident that their changes do not
compromise the system's behavior.

# TODO note that these are largely concerned with OOP->OOP or procedural->OOP


**** Automated approaches

Automated tools that assist with legacy systems are largely concerned
with increasing the knowledge of the system, understanding how it works,
extracting modules, etc. One example
is extracting OOP abstractions such as classes from imperative code,
e.g. by analyzing which functions and variables are used together.
\cite{Etzkorn1997}
\cite{Silva2017}

# TODO this paragraph may be wrong :) read the paper
Another interesting route is creating a "modularization proposal," i.e.
a series of architectural changes to the codebase that lead to a
more modular system while minimizing change each step,
by constructing
a concept lattice based on where different global variables are used.
This lattice can then be used to create descriptions on how to
modularize the codebase. \cite{lindig1997assessing}

Another approach is using automated tools to detect potentially
problematic parts of a codebase, such as overly complex controllers
in GUI applications \cite{Lelli2016},
or code that is simply more likely to be buggy, based on statistical
analyses \cite{Ray2016}. These would help programmers find what parts
of the codebase to target.

# TODO Linters?

# TODO probably something about machine learning

Some ways that developers and researchers have attempted to fix
legacy codebases have been introduced, however one question
still remains: what is it with legacy code that makes it
problematic? Is there some attribute of the code itself, or
is /all/ code that successfully solves a problem doomed to
become "legacy" code? I.e., is it possible to write code that
is "legacy"-resistent?

# TODO REFS need to show that it is in fact possible to write code
# that avoids/reduces the problems. GNU, Linux, etc. would be good.

We have identified that the problems seem to be related to
understanding the code. From that viewpoint, the question becomes: is
there a way to write code that is more easily understandible, and if
so, what characterizes it? This is what the next section seeks to
answer.

*** Code quality

#+begin_comment
Knowing what code is safe to change & knowing what code *to* change
  -> Good code is code that tells the programmer what it does,
     but also what it does *not* do.

Describe code smells wrt. that
#+end_comment

The problems of legacy code revolve around knowing what different
parts of the codebase do; what changes in code lead to what changes
in behavior, and what changes in code do not lead to changes in
a subset of the system behavior. "Good" code could, then, be defined
as code which:

1. Tells the programmer what it does
2. Tells the programmer what it does *not* do

Conversely, "bad" code makes it difficult to see why it exists,
why it is called, and what effects it has, and does not have,
on the system behavior.

These definitions only beg further questions, however, and require
deeper investigation.

**** Knowing what a piece of code does
     means knowing what data it requires to do whatever it is it does,
i.e. what other code it depends on, and knowing what effects it has
on the system behavior.
This may sound simple, but a function or method
can easily grow to the point where it is difficult to see what the
dependencies truly are, e.g. if a compound data type is provided
as the input to a function, is every piece used? If so, how are those
pieces created; what parts of the codebase are truly responsible for the
input to the function? Knowing what code calls the function is not
enough unless the call site is entirely responsible for the input.
# TODO REF, working with legacy code or

While dependencies may be difficult to unravel, the opposite problem
is often much worse.
# TODO REF
Knowing what effects a piece of code has on the system can be extremely
difficult in commonly used languages such as Java and Python,
# TODO REF, why FP matters? something like that
as there is often little limiting what some function or method can
do. A given method may perform a simple calculation on its input
and return the results. It may also perform an HTTP request to some
service and receive the results that way -- with no indication that
the system communicates with the outside world in this manner, nor
that the system /depends/ on said service. It could also modify global
state, which potentially changes the behavior of all other parts of
the codebase that interact with said state, despite there being no
direct interaction between the different parts. This has been called
"spooky action at a distance,"\cite{feathers2004working} a reference
to what Einstein thought of quantum mechanics.
# TODO REF or footnote for Einstein

# TODO keep this or the "spooky action at a distance" quote
#+BEGIN_QUOTE
Most large software systems, even if apparently well-engineered on
a component-by-component basis, have proved to be incoherent as a
whole due to unanticipated long-range "weird interactions" among
supposedly independent parts. \cite{weide1995reverse}
#+END_QUOTE

# TODO add appropriately convoluted example (with misleading function names)
#+begin_comment
#+BEGIN_SRC c
int globalValue = 0;

int rq(int x) {
   globalValue++;
   return x + globalValue;
}

int rt(int x) {
   return x + 1;
}
#+END_SRC
#+end_comment


**** Knowing what a piece of code does not do
# TODO this needs to be reworded & expanded.
#      my point is regarding that knowing that some function F
#      is one place to change the code to produce the desired system change,
#      does *not* imply knowing that said modification *only* produces the desired behavior change.
#      Probably best to bake together this and the previous subsubsection/paragraph
     is, for the above reasons, difficult in many languages. In fact,
while these two questions may appear very similar, it is often not the
case that knowing the answer to one of them lets the programmer deduce
the answer to the other.
# idk if this is right


**** Good code
     must then, somehow, communicate as much information to the programmer
as possible. At the same time, throwing too much information at the programmer
will not help.
# TODO REF cognitive psychology or something; too much shit is bad, obv.
We have seen that legacy code is often if not always imperative code, and
that the OOP paradigm has been an oft-tried solution for legacy code. Despite
this, many current legacy code systems are written in OOP languages such as
Java, and while books provide dozens of ways to write good OOP code, and ways
to reduce the problems of legacy code, there does not seem to be any reason
to believe that OOP is the ultimate solution to legacy code, nor that OOP code
is inherently the best type of code.
# TODO this will require more references and/or argumentation


Object-oriented is not the only way to write code. Functional programming (FP) is
a programming paradigm that takes quite a different approach than OOP, and
purely functional programming provides tools to assist the developer in writing
what we have now defined to be "good" code. The next section provides an
introduction to the area and its ideas.


** Purely functional programming


This section introduces the purely functional programming (pure FP) paradigm,
and its strengths. In short, pure FP enforces "referential transparency,"
which means that any piece of code can be replaced with the value it evaluates
to, wherever it appears in the source code. This makes it easier to understand
the code, any function can be reduced, on a cognitive level, to a black box
that can be passed around and used without having to think about its contents.
\cite{hughes1989functional}

Pure FP achieves this by using immutable data structures, eliminating side effects
in code, and using a powerful type system to express effectful computations
(e.g. code that interacts with the user).
# TODO REFs?


*** Functional programming
# Where it (FP in general) came from (lambda calc)
   # transformations?
The functional paradigm can be seen as a natural extension to the lambda
calculus, a model of computation invented by Alonzo Church, where a
small set of variable binding and substitution rules is used to express
computations.
# TODO REF

# On the other hand,
# imperative programming is closely connected to the von Neumann-style of computer
# on which it runs, and is similar to the idea of a Turing machine.
# TODO REF

# TODO this paragraph could use some more work; give examples of Turing machine-based "languages" e.g. brainfuck
The Turing machine and lambda calculus models of computation are equivalent
\cite{turing_1937}. However, while the Turing machine model is largely
an abstract concept that is useful for modeling computation, but less so
in actually solving programming problems, there are several actively used
languages that are, at their core, some type of lambda calculus. One
example is the purely functional language Haskell. The Glasgow Haskell Compiler,
the de facto standard Haskell implementation, compiles to a language based
on System-F\omega, a typed lambda calculus, as an intermediate language
\cite{haskell2010} \cite{ghcguide}.


# TODO s/main/only/?
The main tool in the lambda calculus is defining and applying functions;
unsurprisingly, functions are the focus of FP. In this case, "function"
is defined in its mathematical sense, as a mapping from inputs to outputs,
rather than the sense of a function in e.g. the C programming language,
where it is rather a series of commands for the computer to execute.

In FP, then, if any function $f$ is given some input $x$ and produces some
output $y$, it must /always/ be the case that $f(x) = y$. Thus, wherever
$f(x)$ (for this given $x$) appears in the code, it can be replaced with $y$,
without changing the program behavior whatsoever. Conversely, if we have
that $g(a) = b$, but calling $g(a)$ prints a value to a console window,
$g$ is /not/ a function in this sense, as replacing $g(a)$ with $b$ in
the code would change the program behavior by not printing to the console.
This characteristic is what is meant by "purity".


*** Purity
# What purity is and what it implies (ref transparency)
   # transformations!



While it is possible to write pure functions in any language that supports
functions, purely functional languages, such as Haskell have features that
ensure that functions are in fact pure. Haskell in particular handles this
through its powerful type system.


# Static types - encoding effects in the types
# Functional programming is orthogonal from type systems, but powerful type
# systems are closely related to pure functional languages.

# Immutability - mutation as side effect
# TODO REF
# is immutability of data. In a purely functional language, functions cannot modify
# data passed to them, as doing so would be to perform a side-effect -- passing the
# same variable to two functions would not necessarily have the result expected
# if one function can modify the input to the other. Using data structures that
# are immutable by default makes reasoning about programs much easier as it removes
# that possible side-effect, no matter the programming paradigm.

# Intro to reading type sigs?
# Monadic IO

# NOTE this is maybe overkill, or could be moved to the PS syntax intro
# For example, Haskell and many(most?) other languages with similar type systems,
# do not have a `null` value, instead encoding the possibility of lacking a value
# in the type system. In Haskell, the type `Maybe` captures this possibility;
# if a function produces an `Int`, you can be sure that after calling the function
# you do indeed have an `Int`.

# TODO maybe something about the saying "if it compiles, it works" and refactoring
    # Other functor-based effects (Maybe vs. null)

# A lower-level part of pure FP, which has seen increased use outside of FP

*** Advantages
# Pure FP/a good lambda-calc based type system gives us even more
  # Category theory as 70 years of docs, abstractions

# WIP Category theory as 70 years of documentation in pure FP languages. (Abstractions, good ones!)
# As a type system gains features, the number of abstractions that can be expressed
# in it increases. Category Theory is a highly abstract branch of mathematics concerned
# with `categories` consisting of `objects` and `morphisms` between objects. It is
# a rich field of research, and has over 70 years of results -- and ideas and abstractions
# from it has been used in programming, especially pure FP. A classic example is
# Haskell's use of `monads`, an abstraction which captures the essence of sequential
# computation.
# TODO REF
# Haskell uses a monadic type for its IO system.
# TODO REF

If a programmer can express their problem in the language of category theory,
they gain access to 70 years of documentation concerning their problem. If the
abstractions used can be expressed in the type system, the compiler can help
prove that the program is correct.

# maybe footnote: for example, everything is an adjunction(TODO REF) and a monoid(TODO REF)

  # Parametric polymorphism & free theorems
    # transformations! e.g. folds, recursion schemes

# Summary of pure FP -- it's all in the types!
    # Summary of pure FP -- transformations, described to the user *and* compiler in the types
# something like "Just a few words, such as (a -> b), should not only tell us what
# the code does, but also tell the compiler what the code should do"

While writing a program in a pure functional language, the programmer
is encouraged by the language and environment to write code that is
reusable and easy to reason about [REF Why functional programming matters].
You also get some level of program correctness, by writing code
using types that correspond to the program semantics. You're able
to construct transformations between data structures and compose
them together -- all type-checked by the compiler.


#+begin_comment
summary of the Background chapter

briefly describe the concepts of legacy code and FP;
  but leave the details of how they interact for the Method section

outro by asking that question -- how do they interact? esp. from an FP viewpoint
#+end_comment


* Method

#+begin_comment
Introduces the method
  i.e. how I went about evaluating pure FP by way of PS as a tool for extending
       legacy code bases and minimizing potential future (legacy) problems of a new code base

First introduce pure FP as a potential tool for legacy code,
looking at specific features and how they correspond to (solving/preventing)
problems

then frame hypothesis

finally present BD & GGB; describe project
#+end_comment


The purpose of this thesis was to evaluate whether pure FP can be a tool for
working with legacy codebases, and if pure FP tends to lead to code
that is less likely to exhibit legacy problems.

This chapter begins with arguments in favor of the hypothesis,
enumerating some features of pure FP that appear advantageous for
limiting the potential problems associated with legacy code. Next, the
programming project that is the core of the thesis, the Graph Genetics
Browser (GGB) is introduced, followed by how the hypothesis was
evaluated in the context of this project.


** Functional programming for legacy codebases
# section needs a better name. later.

#+begin_comment
intro - the various parts of FP we're going to make use of,
  and how they help wrt. the problems related to legacy code


Static types
Abstractions
  Semigroup & monoid
Generative testing (esp. checking typeclass laws)
more..

wrt. assisting the programmer in knowing what code does and does not do
use code smells only as shorthand for specific causes of this
    code duplication --
      more similar pieces of code -> more places to look; less clarity of what to change & effects of change
      e.g. *why* are there multiple similar pieces of code? are they really the same? scope etc.
      DRY principle

    primitive obsession -- less clear
    side effects -- obvious
    mutability -- obvious

summary - by eliminating side effects & giving the programmer tools to know

  pure FP appears to be usable to limit the problems that lead to legacy code problems
#+end_comment

# TODO reorder these paragraphs & quote
Purely functional programming has many tools and features that are likely
to limit the problems of legacy code.

# TODO this quote should fit in somewhere in the Method section
#+BEGIN_QUOTE
I think the lack of reusability comes in object-oriented languages,
not functional languages. Because the problem with object-oriented
languages is they’ve got all this implicit environment that they carry
around with them. You wanted a banana but what you got was a gorilla
holding the banana and the entire jungle.

If you have referentially transparent code, if you have pure functions
— all the data comes in its input arguments and everything goes out
and leave no state behind — it’s incredibly reusable.
--- Joe Armstrong\cite{seibel2009coders}
#+END_QUOTE

This section begins by introducing
the programming language used in the thesis project, and continues with
providing examples of language features that are likely to reduce the
legacy code-related problems and code patterns in general.

*** Introduction to PureScript
PureScript (PS) is a purely functional programming language in the style
of Haskell, that compiles to JavaScript (JS).
# TODO REFS
PS is immutable by default, free of side-effects, and has an advanced
strict static type system that gives the programmer many tools to increase
productivity and program correctness.
# TODO REFS

# Continue by filling in these subsections wrt the above `Code quality` ideas

*** Immutability
Some data being "immutable" means it does not change. PS being "immutable
by default" means that all values that one work with when writing PS code
are immutable --- nothing can change[fn:immutable-caveat]. Instead, if
a function e.g. sorts a list, it creates a new sorted list, rather than
modify the existing list. This ensures that all other parts of the program
that uses the input list continue to function as they did before the sorting
function was called. This would not have to be the case if the list changed
in memory. The programmer never has to think about copying values;
PS takes care of it.

Immutability by default reduces the reach of code by eliminating one
prominent side-effect. It provides the programmer with absolute
certainty that:

# TODO reword these I think
1. inputs to a function are unchanged in that function;
2. calling a function on some value does not change that value.


Mutation is one common side-effect in imperative programming, but far
from the only one. The next section considers effectful programming
in genera

[fn:immutable-caveat] There are some mutable data structures in PS, e.g.
arrays that support in-place mutation for efficiency. These are separate
from the immutable versions; transforming between the two must be done
explicitly. As mutation is an effect, it too is captured by the type system
and must be done in the `Eff` type; see # TODO add ref to Purity section!!

# TODO not sure about the order of these two sections; purity & types are closely connected
*** Purity

A purely functional programming language does not only prevent the
side-effect of mutating data in a function; it prohibits functions
from causing /any/ side-effect[fn:side-effect-caveat]. In pure FP
a function is in truth a function; for a given input it must produce
the same output each and every time it is called. The output includes
all effects the function causes; some examples of effects are now given.

[fn:side-effect-caveat] In PureScript, functions can be made impure
by improper use of the FFI, or by using "unsafe" functions such
as `unsafeCoerce`. This is not encouraged.

A /partial/ function is one that is not defined on all its inputs,
e.g. a function that tries to access an out of bounds index on
an array, whose type is given in block [[code:method-unsafe-index]].
If the given index is outside the array, the function explodes,
for example with an exception. Hardly what the type says it will
do, so `unsafeIndex` is not truly a function.

#+name: code:method-unsafe-index
#+caption:
#+BEGIN_SRC purescript :exports none
unsafeIndex :: forall a. Array a -> Int -> a
#+END_SRC

Another effect is working with implicit state; another, retrieving
data from an external source, or updating the user interface. All of
these effects can be useful, possibly vital, for our programs, so it
would not be desirable to discard them in the name of purity.
Fortunately, PS provides tools to encode effects in the types of
functions, allowing us to write pure yet effectful functions. The next
section gives more details.


*** Type system

PS has a powerful type system that makes it possible to describe much
more of the semantics of the program in a way that the compiler can
describe and check for us.

# TODO simplify this maybe
For example, the effect of partiality can be captured in the type
system with the `Maybe` type, defined in code block [[code:method-maybe]].
A value of type `Maybe Int` can contain either some `Int` wrapped in a
`Just`, or `Nothing`. When writing functions that take a `Maybe` as
input, the PS compiler will ensure that both possibilities are
accounted for by failing with an error if something is missing.

#+name: code:method-maybe
#+caption:
#+BEGIN_SRC purescript :exports none
data Maybe a
  = Just a
  | Nothing
#+END_SRC

With `Maybe` it is possible to write a safe, pure version of `index`,
see block [[code:method-safe-index]]. The effect of potential failure
is captured in the function returning `Maybe a` rather than `a`.

#+name: code:method-safe-index
#+caption:
#+BEGIN_SRC purescript :exports none
index :: forall a. Array a -> Int -> Maybe a
#+END_SRC

# TODO footnote on removing effect rows in 0.12...
In PS, another type is used to encapsulate so-called native effects,
such as printing to the console, updating the UI, etc. This is the
`Eff` type; e.g. if we have a value of type `Eff Int`, we have an
effectful computation that returns an `Int`. It could read from
standard input, perform a fetch from a server, etc. Another effect
covered by `Eff` is local mutation, such as in-place mutation of an
array.


# TODO more on purity in types?
# TODO CT abstractions in types --- monoid etc.

The type system assists in more than purity. For example, PS supports
two powerful forms of polymorphism, typeclasses and parametric
polymorphism.

# TODO this paragraph is bad
# TODO REF
Typeclasses provide ad-hoc polymorphism on types, letting us write
functions on types that satisfy some "constraint" rather than
specifying concrete types. One example is the `Functor` typeclass,
given in code block [[code:method-functor]]. Any function that takes as
argument a type with the `Functor` constraint can use the `map`
function on that argument; conversely, that function can be applied to
any type with a `Functor` instance.

#+name: code:method-functor
#+caption:
#+BEGIN_SRC purescript :exports none
class Functor f where
  map :: forall a b. (a -> b) -> f a -> f b
#+END_SRC

Often typeclasses are used to work with algebraic and
category-theoretic abstractions, which provide a powerful way to write
code that is both general and can be used to write code such that the
compiler can check the semantics of our program in a way that is
relevant to the actual program behavior.
# TODO ref to use of Free monad, monoids, etc. in Results/Glyph section

Parametric polymorphism is reminiscent of generics in languages such
as Java, but more powerful. Consider again the function in block
[[code:method-safe-index]]. The type says that it works on `forall a.
Array a`; meaning it accepts any array, no matter what it contains.
This is what it means for a function to be "parametrically
polymorphic". Besides reducing code duplication by not having to write
one indexing function for `Array Int`, another for `Array Boolean`
etc., parametric polymorphism also provides some additional knowledge
about the function, by enforcing that the function cannot do anything
with some of its arguments other than pass them around. By looking at
the type signature for `index`, we /know/ that the output (if it is
not `Nothing`) must come from the given array, as the function cannot
create values of type `a` from thin air.

A more extreme example is given in code block
[[code:method-identity-fun]]. This is the type of the identity function;
the identity function is in fact the only function with this type.
This is because it is parametrically polymorphic --- the only thing
`id` can do with its argument is return it, there is literally nothing
else that can be done.

#+name: code:method-identity-fun
#+caption:
#+BEGIN_SRC purescript :exports none
id :: forall a. a -> a
#+END_SRC

There is much more to PS' type system, but this covers most of what is
used in the thesis project.


** Extending a Legacy System

#+begin_comment
present BD as legacy code project; including examples of previous work on extending it etc.
present GGB as both extending & a new platform, with intro to purescript

Evaluation of the hypothesis as comparing the resulting code (or a subset of it)
to the code smells and the respective code in BD (where applicable).
If correct -- or at least not obviously incorrect -- the resulting code should
be free of the code smells.
#+end_comment


In this section, the legacy system that was extended --- Biodalliance
--- is described. The extent and nature of the changes, and more
details of the resulting application, are also given.

*** Biodalliance

# Describe BD, its history, etc.

Biodalliance (BD) is an open source (BSD licensed) HTML5-based genome
browser written in JavaScript (JS) \cite{down2011dalliance}. It is
fast, supports several data formats commonly used in bioinformatics,
and the plots displayed can be configured and customized. Since it is
HTML5-based, it can be embedded into any web page and does not require
any special tools to be used. It also supports exporting images as
SVG, which can be used as high quality figures in publications.

# TODO example of difficulties: adding CSV sources; horizontal rulers; /new/ plot types

# TODO make sure GN2 has already been introduced...
# TODO maybe more on why we want these features
For GN2 we want a genome browser that supports these features.
We also want to be able to add new features, however BD has shown itself
to be difficult to work with and extend, for reasons earlier defined
as legacy code problems[fn:gsoc2016-footnote]. BD has been in development
since 2010, and as of December 2017 consists of 17.5k lines of JS code
(sans comments and whitespace) split into 61 files.
# TODO footnote to BD github
The codebase has grown organically over time, and has become complex,
with single pieces of functionality (such as rendering data to the
HTML5 canvas) being split into several large functions in different
files, and less than clear flow of control.

[fn:gsoc2016-footnote] The author previously worked on extending BD as
part of Google Summer of Code 2016, and encountered some difficulties:
https://chfi.se/posts/2016-08-22-gsoc-final.html


We want many of the features of BD, which have required much time and
effort to be implemented. However, we also want new features, but the
time and effort required to implement them in BD is much greater than
necessary due to the legacy aspect of BD's codebase.

# TODO idk about this wording
The solution that was decided upon was to develop a new genome
browser. This browser would allow embedding BD within it, and so be
compatible with BD and support the desired features. However, it would
be a separate application, and be independent of BD's baggage. This
application is the Genetics Graph Browser, to which the next section
is dedicated.


*** Genetics Graph Browser

The Genetics Graph Browser was written in PS. It begun as a tool for
constructing BD-compatible data structures for creating new ways to
plot data, but soon grew to a web application that embeds BD and
Cy.js, and provides communication between the two.
# TODO note that this will be detailed in Results (Glyph)

# TODO give more details on GGB! give a basic overview of the project "as planned"
# TODO add Main features -- i.e. the parts that are in Results

Throughout its development, various legacy-type problems were
encountered in BD, and avoided or solved in GGB. As GGB is written in
PS, this was done using various "features" of FP, from the sections above. In
this way, the GGB project can be seen as a case study in working with
legacy code using purely functional programming.

# TODO this is not great
The main features of GGB include:
+ Working with JS APIs
+ Configuration
+ Units and types
+ Rendering data to the screen
+ Communication between BD and Cy.js
+ Creating a purely functional UI containing a legacy JS app

# TODO Cy.js should already be introduced
Besides wrapping BD and providing genome browser functionality, one of
the goals of GGB is to support exploratory data analysis of both
genome-based data as well as graph-based, semantic web data. Where BD
provides the genome browser, Cytoscape.js (Cy.js), a graph theory tool
written in JS \cite{franz2015cytoscape}, is wrapped to provide the
graph-network functionality.

GGB supports connecting BD and Cy.js data, letting the user configure
interactions between the two browsers based on user interaction (e.g.
updating the Cy.js graph upon clicking on a gene in BD).

The hypothesis of this thesis was then evaluated by examining if and
how pure FP helped with these "legacy problems," both when fixing
problems or improving how a problem was solved in BD, including the
parts of GGB that interact with BD, as well as parts of GGB that are
not related to BD, but solutions to which would likely end up
exhibiting legacy-style problems.

PS was chosen as the language for the project for its support for
interoperation with JS --- PS can simply call JS functions and vice
versa. However, PS also enforces a purely functional programming
style, making it ideal for evaluating the hypothesis.


** Summary

# Summarize how we evaluate the hypothesis with this
The hypothesis, that functional programming techniques can help when
working with an existing legacy code base, as well as lead to code
that is less likely to exhibit legacy code problems in the future,
was tested by developing an application in Purescript that both
interfaces with an existing legacy genome browser, and is intended
to be a stand-alone browser in the future.

# TODO summarize how the various FP features are expected to help

Identifying features whose implementation were already cause for concern in
BD, or had the potential to be implemented in problematic ways in GGB,
and if and how FP helped reduce or eliminate those problems, provides
a lens through which it was possible to make some judgements as to
the validity of the hypothesis.


* Results

#+begin_comment
The results are presented

First an introduction of the "completed" GGB -- its features and scope

Then a walk through the code:
  each section corresponds to one module or part of the GGB, and looks like:
    problems in BD wrt. legacy code (code smells) are presented
    the code in GGB is presented as one way of solving those problems
    (or, present each section as a transformation)

The sections:

  bdcy:   extracting a minimal usable interface from JS APIs
          enforcing safety with types
          mapping JS functions to law-abiding abstractions (Semigroup, Monoid)
      smells:

  config: configuring the browser in a type-safe manner
          validating at launch with type-checking and helpful error messages
          mapping GGB config to BD and Cy.js configuration
      smells:

  units:  type-checking domain-specific operations
          more assistance from the compiler in ensuring semantics of program
          compiler-checked documentation
      smells:

  glyph:  using pure FP abstractions to create an extensible transformation pipeline
            for working with various display and input methods
          semigroups and monoids + generative testing = verification
          creating a language that "compiles" to BD-compatible data structures
            but also is extensible
      smells:

  events: transforming events between "disjoint" JS apps with configuration for free
          exploiting JSON's tree structure for a kind-of type-safe system
      smells:

  ui:     fitting BD and Cy.js into a type-safe UI library;
            tying it all together
          type-safe and extensible, with error-checking from many of the previous sections
      smells:


End with summing up where the various code smells showed up in BD,
and how they were solved with FP/PS in GGB.
#+end_comment



** The Completed Product

#+begin_comment
A brief section serving to provide the reader with an image or two to
anchor the rest of the results section to, and prepare them with a
basic framework to fit the examples to.
#+end_comment

# Present the finished browser!

  # What it looks like

  # How it's used -- show basic functionality with screenshots

  # Figure to refer to concepts such as "tracks", "feature", "glyph" etc.

# Outro


** Interfacing with existing JS
# or: transforming JS APIs to PS APIs (kind of)

#+INCLUDE: "./bdcy.org"


** Safe application configuration
# or: transforming user configuration to safe application options

#+INCLUDE: "./config.org"


** Working with units
# or: domain-based type checking of transformations
#     (or something like that)

#+INCLUDE: "./units.org"


** Transforming data for screen and space

#+INCLUDE: "./glyph.org"


** Transforming events between tracks

#+INCLUDE: "./events.org"


** The User Interface
# or: transformations between user and application (or something like that)

#+INCLUDE: "./ui.org"


** Summary

# Summarize by selecting a few code smells and briefly describing,
# conceptually, how they were removed/avoided/bypassed



* Discussion

The resulting browser, GGB, fills the specified requirements, and the
codebase does as well. Purescript was not a magic bullet, and some
problems were experienced during development, however the source code
on the whole does not, arguably, suffer from the problems of legacy
code as defined in this report.


#+begin_comment

Discuss results -- each of the code parts (subsections from Results)

Does the various FP features/tools from the Method help in producing
"good" code in the Results?

On a higher level, is the resulting codebase "easy" to understand,
and to work with? How difficult is it to understand the overall
architecture, to know what areas to change, and what changes in
system behavior ones' modifications to the code leads to?
  And how much of this can be attributed to FP?
  This will all need to be argued for.

How do the various parts from the Background/FP section fit in?
  Purity
  Static types
  (CT) Abstractions

Discuss refactoring; something that happened many times during the project,
  and something a good type system assists with to a great extent

Discuss future work *on GGB*, i.e. predict how easy it will be to extend;
  how the abstractions will help or hinder this, and why

Concede that it is entirely possible to write "bad" code in PS
also that pure FP and PS have their own antipatterns/code smells
  (though I'm not sure if they're comparable to imperative/OOP ones wrt.
   their effect on codebases)



Sections:
how does it fill the requirements? both browser & codebase (brief)
what problems were encountered during development?
why does it not suffer from legacy problems?
  1. why do we think it does not
  2. why do we think that the reason it does not is due to FP/PS

learning PS during development -- something that will happen "IRL" as well


#+end_comment


The code from the previous chapter is examined to see if and how it
stuck to the ideas of "good" code, including what FP features
contributed. Problems experienced during development are presented,
and some examples of how GGB will be extended in the future are given,
together with estimates on how "easy" the addition of those parts to
the codebase and system will be. The potential ways to extend the
browser are also looked at from the view of BD, to examine how
difficult it would be to extend BD without the support of GGB.



# TODO maybe combine these two following sections
** Purescript as a tool for working with legacy JS

PureScript was an excellent tool for developing GGB; in the author's
opinion, it was no doubt the best choice. PS's FFI capabilities made
it easy to wrap the native JS APIs of BD and Cy.js, then construct
type-safe PS APIs. This minimizes the reach of JS code into the GGB
codebase. Several times during development, when some part of the
system started behaving strangely, it was due to the FFI almost every
time. Even JS code, which due to its untyped dynamic nature
tends to lead to bugs in unexpected places, becomes easy to debug when
restricted to the very edges of the program.

However, it is possible that BD was uncharacteristically simple to
hook into with another application. While the JS API that is exposed
by BD is minimalistic, the "incisions" that had to be made into BD's
source code to make it possible for GGB to add some given functionality,
were few and quite small.

As an example, adding support for external renderers (something
done before this project as a part of GSoC 2016[fn:gsoc2016-renderers]),
while a large undertaking in terms of the time required to understand
what had to be done, and rewriting functions to better understand
the rendering system as a whole, very few changes were made to the
control flow of the rendering system. That is, few, if any, changes
that may have led to undesired changes of system behavior, were required.

[fn:gsoc2016-renderers] The process is detailed in the blog post
at this URL: https://chfi.se/posts/2016-07-26-gsoc-render-complete.html

On the GGB side, this can be seen in the fact that the entire `Glyph`
system of defining new ways of displaying data is mapped to BD by
one relatively small function, seen in code block [[code:glyph-together-1]].

`Glyph` is also an excellent argument for PS and GGB being extensible
and easy to maintain. For example, if a more precise way of checking
whether a glyph covers a given point on the canvas (something currently
done by providing axis-aligned bounding boxes) is desired, it is simple
to create a new interpreter that maps glyphs to some other more exact
collision shape.

# TODO add this example if it ends up looking good (later anyway)
#+begin_comment
#+name: code:discussion-glyph-collision-example
#+caption:
#+BEGIN_SRC purescript :exports none

probably actually want

glyphCoverN :: GlyphF
            ~> Writer (Disj (Tuple Number Number -> Boolean))

not super readable but lol


not this:
glyphCoverN :: GlyphF a -> Const (Tuple Number Number -> Boolean) a
glyphCoverN (Stroke _ a) = pure a
glyphCoverN (Circle p r a) = do
  tell $ GlyphPosition { min: p.x - (r * 1.5)
                       , max: p.x + (r * 1.5)
                       , minY: p.y - (r * 1.5)
                       , maxY: p.y + (r * 1.5)
                       }
  pure a
-- ..
#+END_SRC

#+end_comment

Another potential change that is easily facilitated by `Glyph`'s
construction using the `Free` monad would be to add support for new
ways of rendering the browser, e.g. using WebGL. Write another
interpreter that maps glyphs to whatever the rendering system
requires, and suddenly all existing glyphs (and, by extension, all
functions that e.g. work on collections of glyphs) are compatible with
this new rendering system.

# TODO is it worthwhile to embed JS apps in a PS app
In short, embedding a legacy JS app in a PS app was in this case 
an efficient way of adding achieving the developmental goals of
the project, as interfacing with BD required little overhead.
It helped that work had already been done in that area; other
projects may or may not present difficulties for the approach
taken here.


** Purely functional programming
# ** Functional programming and code quality

Recall that our definition of "good code" is code which is easy
to understand when looked at. Our heuristics for good code include
functions that do not have far-reaching or otherwise difficult to
predict side-effects; conversely, it is also good to be able to
see at a glance what the purpose of some given function is. 

Let us first examine how pure FP has led to functions and code that
more clearly presents their purpose. 

*Configuration* in GGB is
done by parsing a foreign JS object into a record; this record is
then used to initialize the browser itself. 

*Code reuse* is increased by features such as typeclasses and parametric
polymorphism. The use of a `Free` monad DSL in the `Glyph` parts
is a good example of this. 

*Fewer logic problems* thanks to representing values in types that
correspond to their actual units as semantically relevant to the
program behavior, rather than their runtime representation.

*Abstractions* such as `Semigroup`, `Functor`, etc. provide the programmer
with information about what a function does. Code blocks [[code:discussion-concrete-types]]
and [[code:discussion-constraints]] show the type signatures for two
functions, `mystery1` and `mystery2`. The function implementations are 
in fact the same, but the information given in the types is not.

If we look only at `mystery1`, the type give us some idea
of what the function does, but not much. Does it return the first list
in the array? The last? Maybe it creates a list containing the maximum
value of each list in the array. Because the function knows so much
about the types it works on, there is a lot that it can do.

# TODO this could be improved...
On the other hand, see the type of `mystery2`. The only tools at our
disposal are those from the `Foldable` and `Monoid` typeclasses:
from `Foldable`, we know we can walk through the collection, applying a binary function
on the current and next value; from `Monoid`, we have a binary operation
that combines two of the values in the collection, and we can create an
empty value. Code block [[code:discussion-fold]] shows the implementation.

#+name: code:discussion-concrete-types
#+caption: A concretely typed 
#+BEGIN_SRC purescript :exports none
mystery1 :: Array (List Int)
         -> List Int
#+END_SRC

#+name: code:discussion-constraints
#+caption:
#+BEGIN_SRC purescript :exports none
mystery2 :: forall f m.
            Foldable f
         => Monoid m
         -> f m
         -> m
#+END_SRC

# TODO add description of Foldable typeclass?
#+name: code:discussion-fold
#+caption:
#+BEGIN_SRC purescript :exports none
mystery2 = fold
mystery1 = mystery2
#+END_SRC

Next, the impact of pure FP on understanding the consequences of
calling functions. 
One of the reasons that it can be difficlt to see what the effects
of calling some function in an impure language such as JS is that
the function can not only do more or less whatever it wants with
whatever it has at its disposal (which is not necessarily easy to
discover, either), but it can also call other functions with arbitrary
effects, and so on, and so on. The programmer must walk the entire
path of function calls to discover what happens.

Pure FP inverts this, as all effects, be it potential failure in a
function, or updating the browser DOM, or mutating a mutable array,
are encapsulated in the type system. This makes it impossible to
call an effectful function from a pure function. The upshot of this inversion is that code cannot cause far-reaching
consequences; there is none of the "spooky action at a distance" that
is one of the main problems when trying to comprehend legacy code.

** Developmental difficulties

There was not much of a design specification for GGB at the start,
and as the author did not have previous experience with developing
a large pure FP application, there were many design and implementation 
dead-ends on the way to the current state of the product, as it was
difficult to decide on the architecture of the application, and
sometimes how to approach and solve smaller problems. Deciding on
some abstraction too early was a mistake repeatedly made, leading
to refactoring to a more specific implementation. [fn:author-apology]
Often, once the problem was solved, a more general solution showed
itself, and another refactoring was made. Thanks to the type system,
even large refactorings were quite easy, as the compiler largely
ensured that the code at the end of the refactoring effort behaved the
same as before.

[fn:author-apology] Admittedly, the author tended to attempt to use
newly learned (but not necessarily understood) concepts and abstractions
throughout the project. 

Pure FP is a very different beast from imperative programming, not only
when it comes to larger-scale program design. Many of the techniques 
used in this project require considerably more knowledge to understand
than similar solutions in imperative languages. Using typeclass constraints
(`Semigroup` etc.) require the programmer to know their meaning, which
can become overwhelming when a function uses half a dozen or more constraints. [fn:abstraction-apology]
Highly abstract code also tends to become small and terse, which can
also be a problem, especially for newcomers to the language and paradigm.

# TODO maybe could do with a better example (one with more methods, filterable?)
[fn:abstraction-apology] On the other hand, one only needs to learn how
`Semigroup` works once.

This is not to imply that these abstractions etc. must be used to write good
PS code, nor that they are required to gain any of the benefits of
pure FP. They can in fact be ignored, while still reaping most of
the benefits, as immutability, purity, and type-safety are still
provided.

However, it can be difficult to ignore, as one inevitably stumbles
upon them when reading library code --- something that is likely
to occur, as many libraries suffer from having too little documentation.
This is a symptom of PS being such a young language, rather than
some inherent problem with it. Fortunately this problem is mitigated
by PS having a very enthusiastic and welcoming community, filled
with people glad to help newcomers with their problems.


** Summary

# TODO s/GGB was developed/GGB were developed/ ? idk
Pure FP was in many ways a boon in the development of GGB. While
there were problems during development, their impact was limited
by the support of the type system and purity; nor is it likely
that other problems would not have occurred if GGB was developed
using in imperative language and idioms.


* Conclusion


#+begin_comment
Encourage using PS with old JS projects

Case-study nature as a limitation;
will probably want to argue that the results are thanks in large part to the
nature of pure FP and PS; not merely happenstance even though n=1.

Also relatively "easy" because BD is open source and JS; it also has nearly
no external dependencies. Other legacy systems are likely to be much more
difficult to apply this approach to. Java/JVM has some tools and languages;
C or something becomes even more difficult.

Future work:
  Looking at open source projects; is functional OSS "better"?
#+end_comment




** Limitations and Future Work


** Future of GGB

GGB will continue to evolve beyond what has been described in this
report, 

# TODO Native track?
# TODO configuration, dhall
# TODO cytoscape
# TODO extrapolate from ease of refactoring that extensibility & maintenance will be easier



# Bibliography

# \nocite{*}
\bibliographystyle{plainnat}
\bibliography{bibliography}
