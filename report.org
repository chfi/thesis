# -*- eval: (let () (org-babel-goto-named-src-block "setup-export-class") (org-babel-execute-src-block)); -*-

#+TITLE: TODO thesis name
#+AUTHOR: Christian Fischer
#+EMAIL: christian@chfi.se

# Define custom org-latex-class for exporting with correct headers
#+NAME: setup-export-class
#+BEGIN_SRC emacs-lisp :exports none :results silent
  (add-to-list 'org-latex-classes
               '("thesis-report"
                 "\\documentclass{article}
  [DEFAULT-PACKAGES]
  [EXTRA]"
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
  ("\\paragraph{%s}" . "\\paragraph*{%s}")))
#+END_SRC

#+BIND: org-export-filter-plain-text-functions (tmp-f-symbols)
#+BEGIN_SRC emacs-lisp :exports results :results none
(defun tmp-f-symbols (text backend info)
  (when (org-export-derived-backend-p backend 'latex)
        (replace-regexp-in-string "<>" "\Diamond" text)))
#+END_SRC


#+LATEX_CLASS: thesis-report
#+LATEX_CLASS_OPTIONS: [a4paper]


#+LATEX_HEADER: \usepackage[numbers]{natbib}


* Introduction

# TODO Have this here or in Background/Theory?
#+BEGIN_QUOTE
Legacy code. The phrase strikes disgust in the hearts of programmers.
It conjures images of slogging through a murky swamp of tangled
undergrowth with leaches beneath and stinging flies above. It conjures
odors of murk, slime, stagnancy, and offal. Although our first joy of
programming may have been intense, the misery of dealing with legacy
code is often sufficient to extinguish that flame. --- Robert C.
Martin, Foreword to Working Effectively with Legacy Code
\cite{feathers2004working}
#+END_QUOTE

# v. quick intro to legacy code

# overview of the problems caused by legacy code

# v. quick intro to FP

# overview of the problems solved by FP

** Objective
# see if FP can solve legacy code by developing GGB

** Delimitations
# case study

** "Background" [TODO: Rename]
# BD etc., GSoC

** GeneNetwork 2/Affiliations


* Background/Theory
The prerequisite theory for the thesis project is introduced. A definition
of "legacy code" is given, followed by relevant statistics, facts, and
techniques concerning legacy code and working with it. Next, an introduction
to the purely functional programming paradigm <> follows.



** Legacy code
# TODO subsections here could use different names
# Section abstract


*** Definition

# What legacy code is - definition
# Wikipedia defines "legacy system":
# "In computing, a legacy system is an old method, technology, computer system, or
# application program, 'of, relating to, or being a previous or outdated computer
# system.' [..] This can also imply that the system is out of date or in need of
# replacement."

There is no formal definition of "legacy code", but \cite{feathers2004working}
gives the definition "Legacy code is code that we've gotten from someone else."
\citeauthor{Bennett1995} gives another definition, of legacy systems, which
gives more of an idea of what the problem is: "large software systems that we don't know how to cope with but that are vital to
our organization" \cite{Bennett1995}.
Finally, \citeauthor{weide1995reverse} gives a definition closer to the
spirit of the concept as experienced by programmers in the trenches:

#+BEGIN_QUOTE
[..] legacy code, i.e., programs in which too much has been invested
just to throw away but which have proved to be obscure, mysterious,
and brittle in the face of maintenance.
#+END_QUOTE

In other words, legacy code is code that continues to be relevant, e.g.\ by
providing a service that is important, and that requires modification,
or will require modification. If there were never going to be any reason to
modify the code, it would not be worth talking about, nor is it likely that
a system that provides a continually valuable service will not at some point
in the future require maintenance or new features \cite{lehman1980programs}.

# Why legacy code is a problem; i.e. why I should care that there are legacy systems, statistics
  # Num of legacy systems
  # size of legacy systems
  # impact of legacy systems
  # difficulty to change


# TODO statistics on legacy systems; how pervasive are they, how old do they
     # tend to be, how expensive is their maintenance and where do most of
     # the costs fall

For this very reason, legacy systems are prevalent in the world.
If a system works as it should, providing the service that is needed,
and said service must continue to be provided, the safest thing to do
is to leave it as is --- until it is decided, for whatever reason, that
changes must be made.

The U.S. goverment federal IT budget for 2017 was over $89 billion, with
nearly 60% budgeted for operations and maintenance of agency systems,
with a study by the U.S. Government Accountability Office finding that
some federal agencies use systems that are up to 50 years old \cite{gao2016legacy}.

# TODO Footnote The Danger of Legacy Systems https://web.archive.org/web/20120323165836/http://www.mousesecurity.com/?p=220

# TODO point out source code as one problem; not language or OS, since
     # that's basically stable these days (i.e. they're unlikely to
     # be replaced soon, compared to how fast things moved up until
     # the 90s)

Many of these federal agency systems consist of old hardware, old operating
systems, etc., however the problems of a legacy system do not need to be
caused by such factors. The code itself is often the problem \cite{Bisbal1999},
and is what this thesis is concerned with.

# TODO maybe move this paragraph, and maybe the previous one, to a more suitable position
We define a "legacy codebase" to be the codebase of a legacy system,
where the problem of modifying the system is constrained by the code
itself --- the underlying technology is not relevant. Likewise we do not look
at dependencies, a problem solved by pure functional
package managers such as Nix \cite{dolstra2004nix} and Guix \cite{courtes2013functional}.


Why would changes need to be made to a legacy codebase? When the
behavior of the system needs to be changed. \cite{feathers2004working}
identifies four general reasons:

# TODO Maybe add examples?
# - security fixes
# - bug fixes
# - changes to external dependencies
# - changed features
# - new features (e.g. electronic healthcare records)
  1. Adding a feature
  2. Fixing a bug
  3. Improving the design
  4. Optimizing resource usage

All of these somehow modify the behavior of the system; if there was
no change in the system behavior, the change in code must have been
to some part of the codebase that isn't used! Thus, the desired change
requires a change in behavior. The problem with legacy code is that
it is difficult to know how to make the change to the code that produces
this desired change in behavior, and /only/ the desired change.


# Rewrite & use these two paragraphs if more concrete examples are desired
# *maaaybeeee* include this
# WIP The existing data structures etc. can be arbitrarily complex,
#      have old/unused data/fields, be mutated at various places;
#      thus it is difficult to know where to start when inserting
#      new code
# If the system has grown organically over a longer period of time,
# the data structures and procedures that manipulate them have likely
# also grown to fit new features etc., leading to large pieces of
# state that are difficult to reason about. Objects may be doing
# something much different from their original purpose. WIP
# TODO REF

# also *maaaybeeee* include this
# WIP Some new feature/solution to a new problem may involve
#      ""stuff (architecture etc.)"" that the existing code
#      is difficult to fit into, or vice versa
# The changes that need to be made may fundamentally be "out of phase"
# with the existing system. For example, it may require data that
# does exist in some part of the system, but the data is tangled
# up with other state and so on.

# Why legacy code is a *difficult* problem; i.e. how people have tried (and failed) to solve it
# or, equivalently, why it is still a problem

# WIP Why is it difficult to do these things, to change legacy code?
#     Nobody understands the code => the set of changes that are safe
#     to make are unknown


The main reason it is difficult to work with legacy code is lack of
knowledge of the system and codebase, and how the system's behavior
relates to the underlying code. Legacy codebases often lack
documentation and tests, without which a new programmer on the project
has few, if any, tools at their disposal to understand the codebase as
it is, since they do not have any knowledge of how and why the code
came to be as it is. Even if there is a design or system specification
--- which is far from certain --- it is not necessarily accurate. The
code may very well have grown beyond the initial specification, and
the specification need not have been updated in step with the code.

For these reasons, one of the main problems of working with legacy
code is understanding it in the first place \cite{feathers2004working}
\cite{Bennett1995} \cite{siebra2016anticipated}. This is also a
difficult, time-consuming process, and one of the reasons reverse
engineering legacy systems is rarely, if ever, a cost-effective
undertaking \cite{weide1995reverse}. Also according to \citeauthor*{weide1995reverse},
even if a system is successfully reverse engineered and modified,
even if a /new/ system is successfully developed that provides
the same behavior as the legacy system but with a better design,
it is highly likely that the new system, eventually, reaches
a point where it too must be reverse engineered --- where the
new system becomes another legacy system.
# TODO there's a more forceful and better way to end this paragraph...

  # to know why it is still a problem we need to know why it became a problem
  # it "became" a problem when it "became", i.e. when its code was written
  # thus: the problem is bad code.

In short, the problem with legacy code is lack of knowledge in what
the system does, and how the code relates to the system and its parts.
This makes it difficult to know what changes to make to the code to
produce the desired change in system behavior, and if a change made is safe,
i.e.\ that /no/ undesired change in system behavior results.

Not only is working with legacy code difficult and expensive, it may
be the case that doing so will not solve the underlying problems,
dooming the legacy system to a lifetime of being subject to the
swearing of programmers.
# TODO this last sentence must be worded better

One question remain: is there anything special with legacy code,
is /all/ successful code doomed to one day bear the "legacy" label,
or is it possible to write code that avoids, or at least reduces,
the problems associated with legacy code? In other words, what
characterizes the source code of legacy systems, and could code
be written in another way?
# TODO REFS need to show that it is in fact possible to write code
# that avoids/reduces the problems. GNU, Linux, etc. would be good.

  # subsection outro: legacy code problems are caused by bad code being
  #   difficult to work with, extend, etc.

Legacy code is difficult to work with; its dual would be code
that is easy, even pleasant, to work with. Let us call that
code "good", and the code that is characteristic of (but hardly
unique to) the problems related to legacy code, "bad" code.

The question then becomes, what makes code good or bad? This is
what the next section attempts to answer.


*** Code quality

# It's also desirable to know what makes code more or less difficult to
# work with -- both to give a clearer picture of what code to focus on
# when refactoring old systems, as well as give programmers guidelines
# when developing new code.

# begs the question -- what is bad code?
  # limited definition, but:
    # difficult to know what changes to code produce what changes in output (transformation!)
      # i think this is enough, actually. at least if prim. obs./types can fit in here


  # code smells as heuristics for bad code
    # argue that

In a word, we want heuristics for good vs. bad code. These are called code smells, or antipatterns.

# TODO this probably needs... more
First, however, we need to define what is meant by "good" or "bad" code.

Beck & Fowler
# TODO REF
 provides a list of 22 code smells that have been
used (extensively?) since publication.
# TODO REFs
# http://ieeexplore.ieee.org/abstract/document/1235447/
# https://link.springer.com/article/10.1007/s10664-006-9002-8
# https://dl.acm.org/citation.cfm?id=2629648
# http://www.sciencedirect.com/science/article/pii/S0164121215000631


Many of the code smells they list, as well as the solutions to them, are
concerned about class-based object-oriented programming (OOP). OOP has
been the primary programming paradigm for decades (TODO REF),
WIP more here (but what?)



# IDK if this quote fits here, but I want it somewhere!
#+BEGIN_QUOTE
Most large software systems, even if apparently well-engineered on
a component-by-component basis, have proved to be incoherent as a
whole due to unanticipated long-range "weird interactions" among
supposedly independent parts. \cite{weide1995reverse}
#+END_QUOTE

  # what is good code?
    # maybe something about the semantic space of the program vs. the implementation space
      # yeah; good code means it is "easier" to know what some change to the code does to the output,
      # and conversely what change needs to be made to produce desired output
      # transformations!

  # list "our" code smells/heuristics
    # code duplication - special case of shotgun surgery (in most cases)
    # implicit state/mutability
    # side effects
    # primitive obsession
    # shotgun surgery (but with better name)
      # i.e. anything that leads to a change in a small part of the output
        # requiring a disproportionately large change in the program code

- Duplicated code - when a piece of code appears multiple times
  in the codebase, it's a sign of a potential abstraction.
  It also makes it difficult to change things, as the change needs
  to be duplicated several times -- which also leads to more opportunities for mistakes to be made.


- primitive obsession - Using primitive types to represent values that could
  be better represented by composite types or wrapped types. This can
  lead to values being used where they shouldn't be (e.g. providing a
  Number representing a pixel to a function expecting a Number actually
  representing the number of objects in an array). This also makes it
  more difficult to understand what a variable or value is to the program.

- shotgun surgery - When functionality is spread out in the codebase in such a way that
  making a change at one place requires making a change at many other places.
  Reduces code comprehension, as distant pieces of code are somehow interacting,
  and makes it more difficult to modify the codebase successfully.

Simply, a good piece of code makes it clear what it does, how it relates
to the system at large, and how it can be changed or reused without
compromising its behavior or the behavior of the system.


*** Solutions
# then we can move on -- how do we *use* these heuristics

Reverse engineering of legacy code exposed
https://doi.org/10.1145/225014.225045

"Reverse engineering of large legacy software systems generally cannot meet its
objectives because it cannot be cost-effective. [..] it is very costly to
“understand” legacy code sufficiently well to permit changes to be made safely,
because reverse engineering of legacy code is intractable in the usual
computational complexity sense."

As one of the main problems of legacy code is lack of knowledge, one
of the main ways to attempt to solve it is to reverse engineer
the existing system. This is usually done manually [TODO REF], but
can also be done using automatic analysis of the code.

# some short side-paths: modularization, automated code analyses

# TODO subsection or paragraph?
**** Code analysis
One common way is to analyze the code to find ways to modularize
it, to decouple the pieces from one another. This can be done
by OOP stuff

Another interesting route is finding a modularization by constructing
a concept lattice based on where different global variables are used.
This lattice can then be used to create descriptions on how to
modularize the codebase.
# REF: Assessing Modular Structure of Legacy Code Based on Mathematical
#      Concept Analysis
#      https://doi.org/10.1145/253228.253354

# TODO Problems with these approaches, and to automation of this in general
# TODO probably something about machine learning


# the main course: manual reverse engineering

# TODO subsection or paragraph?
**** Manual reverse engineering
this section is not even a section.

The most common
# TODO REF
solution is simply to do it by hand. This
requires that programmers look at, and comprehend, the codebase, and
how the codebase relates to the semantics of the system. Writing tests
should also be done to give greater confidence when changes are made.

# REF The most common way - Do it by hand.

# a quick exploration of the literature/background in this area, OOP etc.

# finally a summary of the legacy code section, especially
# the causes (code smells) followed by

# a smooth segue (ala "there is, however, one paradigm/potential
# solution that has not been examined...")

** Purely functional programming

# Quick intro to what pure FP is; an abstract, basically.
   # transformations

Functional programming as a paradigm focuses on functions in the mathematical
sense, where functions, with some given input, always produces the same output.
In purely functional programming, this concept taken to its limit, with
functions not being able to perform side-effects, such as reading input from the
user, or updating the user interface; more on these actions below. This is in
contrast to the imperative paradigm, which places no such limitations on
functions, other than scoping.
# TODO Footnote: an imperative language could of course
# provide purity, but it's not exactly common, nor is it a natural part of the
# paradigm).

# Then a deep dive into the most important parts

*** Functional programming
# Where it (FP in general) came from (lambda calc)
   # transformations?
The functional paradigm can be seen as a natural extension to the lambda
calculus, a model of computation invented by Alonzo Church,
# TODO REF
while
imperative programming is closely connected to the von Neumann-style of computer
on which it runs, and is similar to the idea of a Turing machine.
# TODO REF(TODO REF)
Whereas a Turing machine models computation as manipulating symbols on an
infinite tape of memory given a set of instructions,
# TODO REF
the lambda
calculus models computation as function abstraction and application; the name
derives from using \lambda to define functions.
# TODO footnote about lambda/anonymous functions, maybe

The Turing machine and lambda calculus models of computation are (probably) equivalent, by the Church-Turing thesis. Thus
any program that can be run on a theoretical Turing machine can be transformed
to "run" in the lambda calculus. However, while programming languages that
are built on the idea of a Turing machine are notoriously difficult to develop
for,
# TODO REF/FOOTNOTE brainfuck, "turing-tarpits"
and are generally interesting only as curiosities or for research,
# TODO REF
languages based on lambda
calculus are more wide-spread. Indeed, the pure functional language Haskell
is at its theoretical core a typed lambda calculus, based on System-F\omega,
which it compiles to as an intermediate language \cite{haskell2010}.
# TODO REF https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/FC


*** Purity
# What purity is and what it implies (ref transparency)
   # transformations!

Pure FP provides something called "referential transparency", which means that
changing a piece of code to the result of running that code does not change the
program.
# TODO REF
This makes "equational reasoning" possible,
# TODO REF
letting the programmer
reason about parts of the program code as separate from the rest of the program.
It gives the programmer confidence in what a function does.

# How pure FP achieves ref transparency, tying back to code smells/good code heuristics
  # Static types - encoding effects in the types
Functional programming is orthogonal from type systems, but powerful type
systems are closely related to pure functional languages.
# TODO REF.
Haskell, being based on a typed lambda calculus, is a statically typed language,

    # Intro to reading type sigs?
    # Monadic IO
and is an example of using a powerful type system
# TODO define
 to capture effects that are
performed by the program -- that is, letting a purely functional language
express effects such as interacting with the real world
# TODO like 2 refs).

Besides capturing effects, a powerful type system provides the programmer with
tools to increase productivity,
# TODO REF
decrease bugs,
# TODO REF
make refactoring easier,
# TODO REF
and improve the programming experience in multiple ways.
# TODO ref & explanation, type-directed search

# NOTE this is maybe overkill, or could be moved to the PS syntax intro
For example, Haskell and many(most?) other languages with similar type systems,
do not have a `null` value, instead encoding the possibility of lacking a value
in the type system. In Haskell, the type `Maybe` captures this possibility;
if a function produces an `Int`, you can be sure that after calling the function
you do indeed have an `Int`.

# TODO maybe something about the saying "if it compiles, it works" and refactoring
    # Other functor-based effects (Maybe vs. null)

  # Immutability - mutation as side effect
A lower-level part of pure FP, which has seen increased use outside of FP(TODO REF)
is immutability of data. In a purely functional language, functions cannot modify
data passed to them, as doing so would be to perform a side-effect -- passing the
same variable to two functions would not necessarily have the result expected
if one function can modify the input to the other. Using data structures that
are immutable by default makes reasoning about programs much easier as it removes
that possible side-effect, no matter the programming paradigm.

*** Advantages
# Pure FP/a good lambda-calc based type system gives us even more
  # Category theory as 70 years of docs, abstractions

# WIP Category theory as 70 years of documentation in pure FP languages. (Abstractions, good ones!)
As a type system gains features, the number of abstractions that can be expressed
in it increases. Category Theory is a highly abstract branch of mathematics concerned
with `categories` consisting of `objects` and `morphisms` between objects. It is
a rich field of research, and has over 70 years of results -- and ideas and abstractions
from it has been used in programming, especially pure FP. A classic example is
Haskell's use of `monads`, an abstraction which captures the essence of sequential
computation.
# TODO REF
Haskell uses a monadic type for its IO system.
# TODO REF

If a programmer can express their problem in the language of category theory,
they gain access to 70 years of documentation concerning their problem. If the
abstractions used can be expressed in the type system, the compiler can help
prove that the program is correct.

# maybe footnote: for example, everything is an adjunction(TODO REF) and a monoid(TODO REF)

  # Parametric polymorphism & free theorems
    # transformations! e.g. folds, recursion schemes

# Summary of pure FP -- it's all in the types!
    # Summary of pure FP -- transformations, described to the user *and* compiler in the types
# something like "Just a few words, such as (a -> b), should not only tell us what
# the code does, but also tell the compiler what the code should do"

While writing a program in a pure functional language, the programmer
is encouraged by the language and environment to write code that is
reusable and easy to reason about [REF Why functional programming matters].
You also get some level of program correctness, by writing code
using types that correspond to the program semantics. You're able
to construct transformations between data structures and compose
them together -- all type-checked by the compiler.

# another smooth segue (zipping the previous two sections together...)
# i.e. "Next it is argued why pure FP can help with the problems of legacy code"


** Functional programming for legacy codebases
# section needs a better name. later.

# intro - the various (apparent) causes for problems of legacy code
  # are analyzed from the viewpoint of pure FP


# code duplication

# primitive obsession

# side effects

# mutability

# summary - by eliminating side effects & giving the programmer tools to know
   # what various parts of the system do,
   # pure FP appears to be usable to limit the problems that lead to legacy code problems



* Method

# TODO I'm not sure about the structure of this section, it feels convoluted;
#      the narrative is unclear to me

#+begin_comment
Introduces the method
  i.e. how I went about evaluating pure FP by way of PS as a tool for extending
       legacy code bases and minimizing potential future (legacy) problems of a new code base

Two questions:
  1. Can pure FP help when working with a legacy code base?
  2. Does writing code with pure FP lead to code that is
     less emblematic of legacy code problems? (i.e. better code)

This was be done by identifying code smells in a legacy code base
and writing a new program that interfaces with the legacy system;
solving or minimizing the impact of said code smells where applicable,
and writing code that avoids recreating the code smells where entirely
new code is necessary.

#+end_comment



** Extending a Legacy System


#+begin_comment
present BD as legacy code project; including examples of previous work on extending it etc.
present GGB as both extending & a new platform, with intro to purescript

Evaluation of the hypothesis as comparing the resulting code (or a subset of it)
to the code smells and the respective code in BD (where applicable).
If correct -- or at least not obviously incorrect -- the resulting code should
be free of the code smells.
#+end_comment


*** Biodalliance

# Describe BD, its history, etc.


*** Genetics Graph Browser
# detail how the product is developed,
# and how it connects to & extends the BD legacy codebase

# list the parts of BD/GGB where legacy-type problems were an issue
  # i.e. the parts that are detailed in results (and why they were chosen)



** Code smells
#+begin_comment
Then present the code smells (briefly if already detailed in Background)

Followed by briefly detailing the components of GGB?
  At least the general goal; it ties in with the
  evaluation and the parts that make up Results.
#+end_comment

#+end_comment



# Introduction to the method - evaluating pure FP as a tool for extending legacy code bases

** Biodalliance
# Describe BD, its history, etc.

** Genetics Graph Browser
# specify the product

# detail how the product is developed,
# and how it connects to & extends the BD legacy codebase

# list the parts of BD/GGB where legacy-type problems were an issue
  # i.e. the parts that are detailed in results (and why they were chosen)

# explain how this answers the research question (case study)


* Results

** Interfacing with existing JS
# or: transforming JS APIs to PS APIs (kind of)

#+INCLUDE: "./bdcy.org" :minlevel 2


** Safe application configuration
# or: transforming user configuration to safe application options

#+INCLUDE: "./config.org" :minlevel 2


** Working with units
# or: domain-based type checking of transformations
#     (or something like that)

#+INCLUDE: "./units.org" :minlevel 2


** Transforming data for screen and space

#+INCLUDE: "./glyph.org" :minlevel 2


** Transforming events between tracks

#+INCLUDE: "./events.org" :minlevel 2


** The User Interface
# or: transformations between user and application (or something like that)

** The Completed Product

# Present the finished browser!

# What it looks like

# How it's used


* Discussion



* Conclusion


# Bibliography

# \nocite{*}
\bibliographystyle{plainnat}
\bibliography{bibliography}
