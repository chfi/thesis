# -*- eval: (let () (org-babel-goto-named-src-block "setup-export-class") (org-babel-execute-src-block)); -*-

#+TITLE: TODO thesis name
#+AUTHOR: Christian Fischer
#+EMAIL: christian@chfi.se

# Define custom org-latex-class for exporting with correct headers
#+NAME: setup-export-class
#+BEGIN_SRC emacs-lisp :exports none :results none
  (add-to-list 'org-latex-classes
               '("thesis-report"
                 "\\documentclass[11pt]{report}"
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
  ("\\paragraph{%s}" . "\\paragraph*{%s}")))
#+END_SRC

#+begin_comment
#+BEGIN_SRC emacs-lisp :exports none :results silent
  (add-to-list 'org-latex-classes
               '("thesis-report" "\\documentclass[11pt]{report}"
  ("\\part{%s}" . "\\part*{%s}")
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}"))
#+END_SRC
#+end_comment

#+BIND: org-export-filter-plain-text-functions (tmp-f-symbols)
#+BEGIN_SRC emacs-lisp :exports results :results none
(defun tmp-f-symbols (text backend info)
  (when (org-export-derived-backend-p backend 'latex)
        (replace-regexp-in-string "<>" "\Diamond" text)))
#+END_SRC


#+LATEX_CLASS: thesis-report
#+LATEX_CLASS_OPTIONS: [a4paper]


#+LATEX_HEADER: \usepackage[numbers]{natbib}
#+OPTIONS: H:6


* Introduction

# TODO Have this here or in Background/Theory?
#+BEGIN_QUOTE
Legacy code. The phrase strikes disgust in the hearts of programmers.
It conjures images of slogging through a murky swamp of tangled
undergrowth with leaches beneath and stinging flies above. It conjures
odors of murk, slime, stagnancy, and offal. Although our first joy of
programming may have been intense, the misery of dealing with legacy
code is often sufficient to extinguish that flame. --- Robert C.
Martin, Foreword to Working Effectively with Legacy Code
\cite{feathers2004working}
#+END_QUOTE

# v. quick intro to legacy code

# overview of the problems caused by legacy code

# v. quick intro to FP

# overview of the problems solved by FP

** Objective
# see if FP can solve legacy code by developing GGB

** Delimitations
# case study

** "Background" [TODO: Rename]
# BD etc., GSoC

** GeneNetwork 2/Affiliations


* Background/Theory
The prerequisite theory for the thesis project is introduced. A definition
of "legacy code" is given, followed by relevant statistics, facts, and
techniques concerning legacy code and working with it. This is followed by an introduction
to the purely functional programming paradigm.



** Legacy code
# TODO subsections here could use different names
# Section abstract


*** Definition

# What legacy code is - definition
# Wikipedia defines "legacy system":
# "In computing, a legacy system is an old method, technology, computer system, or
# application program, 'of, relating to, or being a previous or outdated computer
# system.' [..] This can also imply that the system is out of date or in need of
# replacement."

There is no formal definition of "legacy code", but \cite{feathers2004working}
gives the definition "Legacy code is code that we've gotten from someone else."
\citeauthor{Bennett1995} gives another definition, of legacy systems, which
gives more of an idea of what the problem is: "large software systems that we don't know how to cope with but that are vital to
our organization" \cite{Bennett1995}.
Finally, \citeauthor{weide1995reverse} gives a definition closer to the
spirit of the concept as experienced by programmers in the trenches:

#+BEGIN_QUOTE
[..] legacy code, i.e., programs in which too much has been invested
just to throw away but which have proved to be obscure, mysterious,
and brittle in the face of maintenance.
#+END_QUOTE

In other words, legacy code is code that continues to be relevant, e.g.\ by
providing a service that is important, and that requires modification,
or will require modification. If there were never going to be any reason to
modify the code, it would not be worth talking about, nor is it likely that
a system that provides a continually valuable service will not at some point
in the future require maintenance or new features \cite{lehman1980programs}.

# TODO could maybe do with a bit more
# WIP statistics on legacy systems; how pervasive are they, how old do they
     # tend to be, how expensive is their maintenance and where do most of
     # the costs fall

For this very reason, legacy systems are prevalent in the world.
If a system works as it should, providing the service that is needed,
and said service must continue to be provided, the safest thing to do
is to leave it as is --- until it is decided, for whatever reason, that
changes must be made.

The U.S. goverment federal IT budget for 2017 was over $89 billion, with
nearly 60% budgeted for operations and maintenance of agency systems,
with a study by the U.S. Government Accountability Office finding that
some federal agencies use systems that are up to 50 years old \cite{gao2016legacy}.

# TODO Footnote The Danger of Legacy Systems https://web.archive.org/web/20120323165836/http://www.mousesecurity.com/?p=220


Many of these federal agency systems consist of old hardware, old operating
systems, etc., however the problems of a legacy system do not need to be
caused by such factors. The code itself is often the problem \cite{Bisbal1999},
and is what this thesis is concerned with.

# TODO maybe move this paragraph, and maybe the previous one, to a more suitable position
We define a "legacy codebase" to be the codebase of a legacy system,
where the problem of modifying the system is constrained by the code
itself --- the underlying technology is not relevant. Likewise we do not look
at dependencies, a problem solved by pure functional
package managers such as Nix \cite{dolstra2004nix} and Guix \cite{courtes2013functional}.

Why would changes need to be made to a legacy codebase? When the
behavior of the system needs to be changed. \cite{feathers2004working}
identifies four general reasons:

# TODO Maybe add examples?
# - security fixes
# - bug fixes
# - changes to external dependencies
# - changed features
# - new features (e.g. electronic healthcare records)
  1. Adding a feature
  2. Fixing a bug
  3. Improving the design
  4. Optimizing resource usage

All of these somehow modify the behavior of the system; if there was
no change in the system behavior, the change in code must have been
to some part of the codebase that isn't used! Thus, the desired change
requires a change in behavior. The problem with legacy code is that
it is difficult to know how to make the change to the code that produces
this desired change in behavior, and /only/ the desired change.


# Rewrite & use these two paragraphs if more concrete examples are desired
# *maaaybeeee* include this
# WIP The existing data structures etc. can be arbitrarily complex,
#      have old/unused data/fields, be mutated at various places;
#      thus it is difficult to know where to start when inserting
#      new code
# If the system has grown organically over a longer period of time,
# the data structures and procedures that manipulate them have likely
# also grown to fit new features etc., leading to large pieces of
# state that are difficult to reason about. Objects may be doing
# something much different from their original purpose. WIP
# TODO REF

# also *maaaybeeee* include this
# WIP Some new feature/solution to a new problem may involve
#      ""stuff (architecture etc.)"" that the existing code
#      is difficult to fit into, or vice versa
# The changes that need to be made may fundamentally be "out of phase"
# with the existing system. For example, it may require data that
# does exist in some part of the system, but the data is tangled
# up with other state and so on.

# Why legacy code is a *difficult* problem; i.e. how people have tried (and failed) to solve it
# or, equivalently, why it is still a problem

# WIP Why is it difficult to do these things, to change legacy code?
#     Nobody understands the code => the set of changes that are safe
#     to make are unknown


The main reason it is difficult to work with legacy code is lack of
knowledge of the system and codebase, and how the system's behavior
relates to the underlying code. Legacy codebases often lack
documentation and tests, without which a new programmer on the project
has few, if any, tools at their disposal to understand the codebase as
it is, since they do not have any knowledge of how and why the code
came to be as it is. Even if there is a design or system specification
--- which is far from certain --- it is not necessarily accurate. The
code may very well have grown beyond the initial specification, and
the specification need not have been updated in step with the code.

For these reasons, one of the main problems of working with legacy
code is understanding it in the first place \cite{feathers2004working}
\cite{Bennett1995} \cite{siebra2016anticipated}. This is also a
difficult, time-consuming process, and one of the reasons reverse
engineering legacy systems is rarely, if ever, a cost-effective
undertaking \cite{weide1995reverse}. Also according to \citeauthor*{weide1995reverse},
even if a system is successfully reverse engineered and modified,
even if a /new/ system is successfully developed that provides
the same behavior as the legacy system but with a better design,
it is highly likely that the new system, eventually, reaches
a point where it too must be reverse engineered --- where the
new system becomes another legacy system.
# TODO there's a more forceful and better way to end this paragraph...

  # to know why it is still a problem we need to know why it became a problem
  # it "became" a problem when it "became", i.e. when its code was written
  # thus: the problem is bad code.

In short, the problem with legacy code is lack of knowledge in what
the system does, and how the code relates to the system and its parts.
This makes it difficult to know what changes to make to the code to
produce the desired change in system behavior, and if a change made is safe,
i.e.\ that /no/ undesired change in system behavior results.


*** Solutions

Legacy code has been a recognized problem for decades, and people have
been trying to solve it for as long.
# TODO REF


Reverse engineering of legacy code exposed
https://doi.org/10.1145/225014.225045

"Reverse engineering of large legacy software systems generally cannot meet its
objectives because it cannot be cost-effective. [..] it is very costly to
“understand” legacy code sufficiently well to permit changes to be made safely,
because reverse engineering of legacy code is intractable in the usual
computational complexity sense."

As one of the main problems of legacy code is lack of knowledge, one
of the main ways to attempt to solve it is to reverse engineer
the existing system. This is usually done manually,
# TODO REF
but can also be done using automatic analysis of the code.

# some short side-paths: modularization, automated code analyses

# TODO subsection or paragraph?
**** Code analysis
One common way is to analyze the code to find ways to modularize
it, to decouple the pieces from one another. This can be done
by OOP stuff

Another interesting route is finding a modularization by constructing
a concept lattice based on where different global variables are used.
This lattice can then be used to create descriptions on how to
modularize the codebase.
# REF: Assessing Modular Structure of Legacy Code Based on Mathematical
#      Concept Analysis
#      https://doi.org/10.1145/253228.253354

# TODO Problems with these approaches, and to automation of this in general
# TODO probably something about machine learning


# the main course: manual reverse engineering

# TODO subsection or paragraph?
**** Manual reverse engineering
this section is not even a section.

The most common
# TODO REF
solution is simply to do it by hand. This
requires that programmers look at, and comprehend, the codebase, and
how the codebase relates to the semantics of the system. Writing tests
should also be done to give greater confidence when changes are made.

# REF The most common way - Do it by hand.

# a quick exploration of the literature/background in this area, OOP etc.

# finally a summary of the legacy code section, especially
# the causes (code smells) followed by

Not only is working with legacy code difficult and expensive, it may
be the case that doing so will not solve the underlying problems,
dooming the legacy system to a lifetime of being subject to the
swearing of programmers.
# TODO this last sentence must be worded better

One question remains: is there anything special with legacy code,
is /all/ successful code doomed to one day bear the "legacy" label,
or is it possible to write code that avoids, or at least reduces,
the problems associated with legacy code? In other words, what
characterizes the source code of legacy systems, and could code
be written in another way?
# TODO REFS need to show that it is in fact possible to write code
# that avoids/reduces the problems. GNU, Linux, etc. would be good.

  # subsection outro: legacy code problems are caused by bad code being
  #   difficult to work with, extend, etc.

Legacy code is difficult to work with; its dual would be code
that is easy, even pleasant, to work with. Let us call that
code "good", and the code that is characteristic of (but hardly
unique to) the problems related to legacy code, "bad" code.

The question then becomes, what makes code good or bad? This is
what the next section attempts to answer.


*** Code quality

#+begin_comment
Knowing what code is safe to change & knowing what code *to* change
  -> Good code is code that tells the programmer what it does,
     but also what it does *not* do.

Describe code smells wrt. that
#+end_comment

The problems of legacy code revolve around knowing what different
parts of the codebase do; what changes in code lead to what changes
in behavior, and what changes in code do not lead to changes in
a subset of the system behavior. "Good" code could, then, be defined
as code which:

1. Tells the programmer what it does
2. Tells the programmer what it does *not* do

Conversely, "bad" code makes it difficult to see why it exists,
why it is called, and what effects it has, and does not have,
on the system behavior.

These definitions only beg further questions, however, and require
deeper investigation.

**** Knowing what a piece of code does
     means knowing what data it requires to do whatever it is it does,
i.e. what other code it depends on, and knowing what effects it has
on the system behavior.
This may sound simple, but a function or method
can easily grow to the point where it is difficult to see what the
dependencies truly are, e.g. if a compound data type is provided
as the input to a function, is every piece used? If so, how are those
pieces created; what parts of the codebase are truly responsible for the
input to the function? Knowing what code calls the function is not
enough unless the call site is entirely responsible for the input.
# TODO REF, working with legacy code or

While dependencies may be difficult to unravel, the opposite problem
is often much worse.
# TODO REF
Knowing what effects a piece of code has on the system can be extremely
difficult in commonly used languages such as Java and Python,
# TODO REF, why FP matters? something like that
as there is often little limiting what some function or method can
do. A given method may perform a simple calculation on its input
and return the results. It may also perform an HTTP request to some
service and receive the results that way -- with no indication that
the system communicates with the outside world in this manner, nor
that the system /depends/ on said service. It could also modify global
state, which potentially changes the behavior of all other parts of
the codebase that interact with said state, despite there being no
direct interaction between the different parts. This has been called
"spooky action at a distance,"\cite{feathers2004working} a reference
to what Einstein thought of quantum mechanics.
# TODO REF or footnote for Einstein

# TODO keep this or the "spooky action at a distance" quote
#+BEGIN_QUOTE
Most large software systems, even if apparently well-engineered on
a component-by-component basis, have proved to be incoherent as a
whole due to unanticipated long-range "weird interactions" among
supposedly independent parts. \cite{weide1995reverse}
#+END_QUOTE

# TODO add appropriately convoluted example (with misleading function names)
#+begin_comment
#+BEGIN_SRC c
int globalValue = 0;

int rq(int x) {
   globalValue++;
   return x + globalValue;
}

int rt(int x) {
   return x + 1;
}
#+END_SRC
#+end_comment


**** Knowing what a piece of code does not do
# TODO this needs to be reworded & expanded.
#      my point is regarding that knowing that some function F
#      is one place to change the code to produce the desired system change,
#      does *not* imply knowing that said modification *only* produces the desired behavior change.
#      Probably best to bake together this and the previous subsubsection/paragraph
     is, for the above reasons, difficult in many languages. In fact,
while these two questions may appear very similar, it is often not the
case that knowing the answer to one of them lets the programmer deduce
the answer to the other.
# idk if this is right


**** Good code
     must then, somehow, communicate as much information to the programmer
as possible. At the same time, throwing too much information at the programmer
will not help.
# TODO REF cognitive psychology or something; too much shit is bad, obv.
We have seen that legacy code is often if not always imperative code, and
that the OOP paradigm has been an oft-tried solution for legacy code. Despite
this, many current legacy code systems are written in OOP languages such as
Java, and while books provide dozens of ways to write good OOP code, and ways
to reduce the problems of legacy code, there does not seem to be any reason
to believe that OOP is the ultimate solution to legacy code, nor that OOP code
is inherently the best type of code.
# TODO this will require more references and/or argumentation


Object-oriented is not the only way to write code. Functional programming (FP) is
a programming paradigm that takes quite a different approach than OOP, and
purely functional programming provides tools to assist the developer in writing
what we have now defined to be "good" code. The next section provides an
introduction to the area and its ideas.


** Purely functional programming

# Quick intro to what pure FP is; an abstract, basically.
   # transformations

Functional programming as a paradigm focuses on functions in the mathematical
sense, where functions, with some given input, always produces the same output.
In purely functional programming, this concept taken to its limit, with
functions not being able to perform side-effects, such as reading input from the
user, or updating the user interface; more on these actions below. This is in
contrast to the imperative paradigm, which places no such limitations on
functions, other than scoping.
# TODO Footnote: an imperative language could of course
# provide purity, but it's not exactly common, nor is it a natural part of the
# paradigm). Take Rust as an example of an imperative language limiting side effects & mutability


# Then a deep dive into the most important parts

*** Functional programming
# Where it (FP in general) came from (lambda calc)
   # transformations?
The functional paradigm can be seen as a natural extension to the lambda
calculus, a model of computation invented by Alonzo Church,
# TODO REF
while
imperative programming is closely connected to the von Neumann-style of computer
on which it runs, and is similar to the idea of a Turing machine.
# TODO REF(TODO REF)
Whereas a Turing machine models computation as manipulating symbols on an
infinite tape of memory given a set of instructions,
# TODO REF
the lambda
calculus models computation as function abstraction and application; the name
derives from using \lambda to define functions.
# TODO footnote about lambda/anonymous functions, maybe

The Turing machine and lambda calculus models of computation are (probably) equivalent, by the Church-Turing thesis. Thus
any program that can be run on a theoretical Turing machine can be transformed
to "run" in the lambda calculus. However, while programming languages that
are built on the idea of a Turing machine are notoriously difficult to develop
for,
# TODO REF/FOOTNOTE brainfuck, "turing-tarpits"
and are generally interesting only as curiosities or for research,
# TODO REF
languages based on lambda
calculus are more wide-spread. Indeed, the pure functional language Haskell
is at its theoretical core a typed lambda calculus, based on System-F\omega,
which it compiles to as an intermediate language \cite{haskell2010}.
# TODO REF https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/FC


*** Purity
# What purity is and what it implies (ref transparency)
   # transformations!

Pure FP provides something called "referential transparency", which means that
changing a piece of code to the result of running that code does not change the
program.
# TODO REF
This makes "equational reasoning" possible,
# TODO REF
letting the programmer
reason about parts of the program code as separate from the rest of the program.
It gives the programmer confidence in what a function does.

# How pure FP achieves ref transparency, tying back to code smells/good code heuristics
  # Static types - encoding effects in the types
Functional programming is orthogonal from type systems, but powerful type
systems are closely related to pure functional languages.
# TODO REF.
Haskell, being based on a typed lambda calculus, is a statically typed language,

    # Intro to reading type sigs?
    # Monadic IO
and is an example of using a powerful type system
# TODO define
 to capture effects that are
performed by the program -- that is, letting a purely functional language
express effects such as interacting with the real world
# TODO like 2 refs).

Besides capturing effects, a powerful type system provides the programmer with
tools to increase productivity,
# TODO REF
decrease bugs,
# TODO REF
make refactoring easier,
# TODO REF
and improve the programming experience in multiple ways.
# TODO ref & explanation, type-directed search

# NOTE this is maybe overkill, or could be moved to the PS syntax intro
For example, Haskell and many(most?) other languages with similar type systems,
do not have a `null` value, instead encoding the possibility of lacking a value
in the type system. In Haskell, the type `Maybe` captures this possibility;
if a function produces an `Int`, you can be sure that after calling the function
you do indeed have an `Int`.

# TODO maybe something about the saying "if it compiles, it works" and refactoring
    # Other functor-based effects (Maybe vs. null)

  # Immutability - mutation as side effect
A lower-level part of pure FP, which has seen increased use outside of FP
# TODO REF
is immutability of data. In a purely functional language, functions cannot modify
data passed to them, as doing so would be to perform a side-effect -- passing the
same variable to two functions would not necessarily have the result expected
if one function can modify the input to the other. Using data structures that
are immutable by default makes reasoning about programs much easier as it removes
that possible side-effect, no matter the programming paradigm.

*** Advantages
# Pure FP/a good lambda-calc based type system gives us even more
  # Category theory as 70 years of docs, abstractions

# WIP Category theory as 70 years of documentation in pure FP languages. (Abstractions, good ones!)
As a type system gains features, the number of abstractions that can be expressed
in it increases. Category Theory is a highly abstract branch of mathematics concerned
with `categories` consisting of `objects` and `morphisms` between objects. It is
a rich field of research, and has over 70 years of results -- and ideas and abstractions
from it has been used in programming, especially pure FP. A classic example is
Haskell's use of `monads`, an abstraction which captures the essence of sequential
computation.
# TODO REF
Haskell uses a monadic type for its IO system.
# TODO REF

If a programmer can express their problem in the language of category theory,
they gain access to 70 years of documentation concerning their problem. If the
abstractions used can be expressed in the type system, the compiler can help
prove that the program is correct.

# maybe footnote: for example, everything is an adjunction(TODO REF) and a monoid(TODO REF)

  # Parametric polymorphism & free theorems
    # transformations! e.g. folds, recursion schemes

# Summary of pure FP -- it's all in the types!
    # Summary of pure FP -- transformations, described to the user *and* compiler in the types
# something like "Just a few words, such as (a -> b), should not only tell us what
# the code does, but also tell the compiler what the code should do"

While writing a program in a pure functional language, the programmer
is encouraged by the language and environment to write code that is
reusable and easy to reason about [REF Why functional programming matters].
You also get some level of program correctness, by writing code
using types that correspond to the program semantics. You're able
to construct transformations between data structures and compose
them together -- all type-checked by the compiler.


#+begin_comment
summary of the Background chapter

briefly describe the concepts of legacy code and FP;
  but leave the details of how they interact for the Method section

outro by asking that question -- how do they interact? esp. from an FP viewpoint
#+end_comment


* Method

#+begin_comment
Introduces the method
  i.e. how I went about evaluating pure FP by way of PS as a tool for extending
       legacy code bases and minimizing potential future (legacy) problems of a new code base

First introduce pure FP as a potential tool for legacy code,
looking at specific features and how they correspond to (solving/preventing)
problems

then frame hypothesis

finally present BD & GGB; describe project


#+end_comment

# TODO this quote should fit in somewhere in the Method section
#+BEGIN_QUOTE
I think the lack of reusability comes in object-oriented languages,
not functional languages. Because the problem with object-oriented
languages is they’ve got all this implicit environment that they carry
around with them. You wanted a banana but what you got was a gorilla
holding the banana and the entire jungle.

If you have referentially transparent code, if you have pure functions
— all the data comes in its input arguments and everything goes out
and leave no state behind — it’s incredibly reusable.
--- Joe Armstrong\cite{seibel2009coders}
#+END_QUOTE



** Functional programming for legacy codebases
# section needs a better name. later.

#+begin_comment
intro - the various parts of FP we're going to make use of,
  and how they help wrt. the problems related to legacy code


Static types
Abstractions
  Semigroup & monoid
Generative testing (esp. checking typeclass laws)
more..

wrt. assisting the programmer in knowing what code does and does not do
use code smells only as shorthand for specific causes of this
    code duplication --
      more similar pieces of code -> more places to look; less clarity of what to change & effects of change
      e.g. *why* are there multiple similar pieces of code? are they really the same? scope etc.

    primitive obsession -- less clear
    side effects -- obvious
    mutability -- obvious

summary - by eliminating side effects & giving the programmer tools to know
  what various parts of the system do,
  pure FP appears to be usable to limit the problems that lead to legacy code problems

#+end_comment

** Extending a Legacy System


#+begin_comment
present BD as legacy code project; including examples of previous work on extending it etc.
present GGB as both extending & a new platform, with intro to purescript

Evaluation of the hypothesis as comparing the resulting code (or a subset of it)
to the code smells and the respective code in BD (where applicable).
If correct -- or at least not obviously incorrect -- the resulting code should
be free of the code smells.
#+end_comment


*** Biodalliance

# Describe BD, its history, etc.


*** Genetics Graph Browser
# detail how the product is developed,
# and how it connects to & extends the BD legacy codebase

# list the parts of BD/GGB where legacy-type problems were an issue
  # i.e. the parts that are detailed in results (and why they were chosen)



# NOTE actually we probably don't need the code smells in Method at all
#+begin_comment

** Code smells
Then present the code smells (briefly if already detailed in Background)

Followed by briefly detailing the components of GGB?
  At least the general goal; it ties in with the
  evaluation and the parts that make up Results.
#+end_comment


* Results

#+begin_comment
The results are presented

First an introduction of the "completed" GGB -- its features and scope

Then a walk through the code:
  each section corresponds to one module or part of the GGB, and looks like:
    problems in BD wrt. legacy code (code smells) are presented
    the code in GGB is presented as one way of solving those problems
    (or, present each section as a transformation)

The sections:

  bdcy:   extracting a minimal usable interface from JS APIs
          enforcing safety with types
          mapping JS functions to law-abiding abstractions (Semigroup, Monoid)
      smells:

  config: configuring the browser in a type-safe manner
          validating at launch with type-checking and helpful error messages
          mapping GGB config to BD and Cy.js configuration
      smells:

  units:  type-checking domain-specific operations
          more assistance from the compiler in ensuring semantics of program
          compiler-checked documentation
      smells:

  glyph:  using pure FP abstractions to create an extensible transformation pipeline
            for working with various display and input methods
          semigroups and monoids + generative testing = verification
          creating a language that "compiles" to BD-compatible data structures
            but also is extensible
      smells:

  events: transforming events between "disjoint" JS apps with configuration for free
          exploiting JSON's tree structure for a kind-of type-safe system
      smells:

  ui:     fitting BD and Cy.js into a type-safe UI library;
            tying it all together
          type-safe and extensible, with error-checking from many of the previous sections
      smells:


End with summing up where the various code smells showed up in BD,
and how they were solved with FP/PS in GGB.
#+end_comment



** The Completed Product

#+begin_comment
A brief section serving to provide the reader with an image or two to
anchor the rest of the results section to, and prepare them with a
basic framework to fit the examples to.
#+end_comment

# Present the finished browser!

  # What it looks like

  # How it's used -- show basic functionality with screenshots

  # Figure to refer to concepts such as "tracks", "feature", "glyph" etc.

# Outro


** Interfacing with existing JS
# or: transforming JS APIs to PS APIs (kind of)

#+INCLUDE: "./bdcy.org"


** Safe application configuration
# or: transforming user configuration to safe application options

#+INCLUDE: "./config.org"


** Working with units
# or: domain-based type checking of transformations
#     (or something like that)

#+INCLUDE: "./units.org"


** Transforming data for screen and space

#+INCLUDE: "./glyph.org"


** Transforming events between tracks

#+INCLUDE: "./events.org"


** The User Interface
# or: transformations between user and application (or something like that)

#+INCLUDE: "./ui.org"


** Summary

# Summarize by selecting a few code smells and briefly describing,
# conceptually, how they were removed/avoided/bypassed


* Discussion


#+begin_comment

Discuss results -- each of the code parts (subsections from Results)

Does the various FP features/tools from the Method help in producing
"good" code in the Results?

On a higher level, is the resulting codebase "easy" to understand,
and to work with? How difficult is it to understand the overall
architecture, to know what areas to change, and what changes in
system behavior ones' modifications to the code leads to?
  And how much of this can be attributed to FP?
  This will all need to be argued for.

How do the various parts from the Background/FP section fit in?
  Purity
  Static types
  (CT) Abstractions

Discuss refactoring; something that happened many times during the project,
  and something a good type system assists with to a great extent

Discuss future work *on GGB*, i.e. predict how easy it will be to extend;
  how the abstractions will help or hinder this, and why

Concede that it is entirely possible to write "bad" code in PS
also that pure FP and PS have their own antipatterns/code smells
  (though I'm not sure if they're comparable to imperative/OOP ones wrt.
   their effect on codebases)
#+end_comment


* Conclusion


#+begin_comment
Case-study nature as a limitation;
will probably want to argue that the results are thanks in large part to the
nature of pure FP and PS; not merely happenstance even though n=1.

Also relatively "easy" because BD is open source and JS; it also has nearly
no external dependencies. Other legacy systems are likely to be much more
difficult to apply this approach to. Java/JVM has some tools and languages;
C or something becomes even more difficult.

Future work:
  Looking at open source projects; is functional OSS "better"?
#+end_comment


** Limitations and Future Work


# Bibliography

# \nocite{*}
\bibliographystyle{plainnat}
\bibliography{bibliography}
