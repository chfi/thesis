# -*- eval: (let () (org-babel-goto-named-src-block "setup-export-class") (org-babel-execute-src-block)); -*-

# Define custom org-latex-class for exporting with correct headers
#+NAME: setup-export-class
#+BEGIN_SRC emacs-lisp :exports none :results none
  (add-to-list 'org-latex-classes
               '("thesis-report"
                 "\\documentclass[11pt]{report}"
  ("\\chapter{%s}" . "\\chapter*{%s}")
  ("\\section{%s}" . "\\section*{%s}")
  ("\\subsection{%s}" . "\\subsection*{%s}")
  ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
  ("\\paragraph{%s}" . "\\paragraph*{%s}")))
#+END_SRC

#+BIND: org-export-filter-plain-text-functions (tmp-f-symbols)
#+BEGIN_SRC emacs-lisp :exports results :results none
(defun tmp-f-symbols (text backend info)
  (when (org-export-derived-backend-p backend 'latex)
        (replace-regexp-in-string "<>" "\Diamond" text)))
#+END_SRC


#+LATEX_CLASS: thesis-report
#+LATEX_CLASS_OPTIONS: [a4paper,11pt,twoside]

# # Standard English
# #+LATEX_HEADER: \usepackage{UmUThesis}

# Non indented English
#+LATEX_HEADER: \usepackage[noindent]{UmUThesis}

# Nicer fonts are used. (not necessary)
# \usepackage{courier}
# Also nicer fonts. (not necessary)
# \usepackage{pslatex}

#+LATEX_HEADER: \usepackage[numbers]{natbib}

#+LATEX_HEADER: \interfootnotelinepenalty=10000
#+LATEX_HEADER: \setlength{\skip\footins}{1.5cm}
#+LATEX_HEADER: \setlength{\footnotesep}{0.5cm}

# code listing paddings
#+LATEX_HEADER: \setlength\partopsep{-\topsep}
#+LATEX_HEADER: \addtolength\partopsep{-\parskip}
#+LATEX_HEADER: \addtolength\partopsep{0.75cm}


# \title{The title of your thesis}

#+TITLE: Extending Legacy Software with Functional Programming
#+SUBTITLE: Using PureScript to Improve Legacy JavaScript
# \subtitle{If you have a subtitle}

#+AUTHOR: Christian Fischer
#+EMAIL: christian@chfi.se
#+LATEX_HEADER: \supervisor{Anders Broberg}
#+LATEX_HEADER: \supervisore{}
#+LATEX_HEADER: \examiner{Ola Ringdahl}
#+LATEX_HEADER: \semester{HT 2017}
#+LATEX_HEADER: \course{Master's Thesis in Interaction Technology and Design, 30 credits}

\pagestyle{empty}

#+OPTIONS: toc:nil
#+OPTIONS: H:6

#+begin_abstract
Legacy systems are everywhere. Immense resources are placed on fixing
problems caused by them, and on legacy system maintenance and reverse
engineering. After decades of research, a solution has yet to be
found. In this thesis, both the viability of using purely functional
programming to mitigate problems of legacy systems is investigated, as
well as the possibility that purely functional programming can lead to
code that is less likely to lead to legacy problems in the first
place. This was done by developing a genome browser in PureScript that
embeds, interfaces with, and extends, an existing genome browser
written in JavaScript. The resulting codebase is examined, and various
characteristics of purely functional programming, and how they helped
solve or avoid problems related to legacy systems, are presented. In
Conclusion, PureScript is found to be an excellent tool for working
with legacy JavaScript, and while the nature of the project limits the
conclusions that can be drawn, it appears likely that using purely
functional programming, especially with a language such as PureScript
that provides a powerful type-system for ensuring program correctness,
leads to code that is more easily understandable, and thus avoids the
problems of legacy code.
#+end_abstract


\cleardoublepage

** Acknowledgements
:PROPERTIES:
:UNNUMBERED: t
:END:
This thesis would not have been possible without the support and
patience of my supervisor and mentor, Pjotr Prins. I am also grateful
for the help of my university supervisor, Anders Broberg, for vital
help especially in structuring this report. Last but not least, I
would like to thank Professor Rob Williams for providing me with the
opportunity to present my work-in-progress at Complex Trait Community
in Memphis, TN, June 2017.



#+TOC: headlines 3

* Introduction

#+BEGIN_QUOTE
Legacy code. The phrase strikes disgust in the hearts of programmers.
It conjures images of slogging through a murky swamp of tangled
undergrowth with leaches beneath and stinging flies above. It conjures
odors of murk, slime, stagnancy, and offal. Although our first joy of
programming may have been intense, the misery of dealing with legacy
code is often sufficient to extinguish that flame.

--- Robert C. Martin \cite{feathers2004working}
#+END_QUOTE

Problems related to and caused by /legacy code/ are ubiquitous.
Maintenance of legacy systems costs billions of US dollars every year,
and as time goes on, the number of legacy systems in production will
continue growing, as what is considered modern today is likely to
become "legacy" tomorrow. In fact, one does not need to go back far in
time to find "legacy" code, as any code that is difficult to
understand, and it is difficult to make changes to such that the
effects of the changes on the system are predictable, can be
considered "legacy" code \cite{Bennett1995}.

Many solutions have been proposed and tried, yet legacy code continues
to be a problem. It will continue to be a problem, and be solved not
not when a way to "repair" legacy code is found, but when a way to
write code that does not /become/ legacy code is
found \cite{weide1995reverse}.

This thesis is concerned with evaluating purely functional programming
as one potential solution to and preventative measure against legacy
code problems. This was done by developing a web application in
PureScript (PS) that both embeds an existing JavaScript (JS)
application, as well as provides novel functionality. The process, and
resulting code, were evaluated by looking for signs of legacy code
problems, as defined in the Method chapter.


** Context

GeneNetwork 2 (GN2) is a web-based database and toolset for doing
genetics online \cite{mulligan2017genenetwork}, developed at University
of Tennessee Health Science Center. One feature yet to be implemented
in GN2 is an interactive genome browser.

Biodalliance (BD) is one such genome browser, which is written in
JS and can be embedded in web
pages \cite{down2011dalliance}. However, BD is several years old, and
has accumulated a lot of code and features, which have led to
difficulties when adding new functionality that is desired in GN2.

The Graph Genetics Browser (GGB) was developed as part of this thesis
to solve these problems, by embedding BD and wrapping the desired
features of BD in a new interface, as well as providing the groundwork
for a new genome browser independent of BD. The development of GGB
served to evaluate the hypothesis, described next.

** Objective

The objective of this thesis is to evaluate the use of purely functional
programming as a tool to:

1. Work with and extend an existing legacy system
2. Develop an application unlikely to develop problems associated with legacy systems

The legacy system in question was BD, the new application GGB. A BD browser
is embedded in GGB, so that the functionality of BD is gained while
providing a framework for a new browser without the legacy baggage of
BD. As initial new functionality, the Cytoscape.js (Cy.js) graph
network browser is also embedded, with support for some interaction
between Cy.js and BD, managed by GGB.

The development of GGB provides an opportunity to evaluate the
hypothesis that purely functional programming can assist when working
with legacy systems, as well as lead to code that is less likely to
exhibit legacy problems. This is done by identifying some general
causes of legacy problems, and looking at the resulting codebase. Of
course, this thesis concerns a single programming project and a single
programmer; it can be seen as a case study, with all the limitations
inherent to case studies.

** Report structure

This report begins with presenting, in the Theory chapter, the concept
of legacy code, why and how it is a problem, and how the problems
presented by legacy code have been attempted to be solved previously.
The problems are distilled into heuristics for code quality, as tools
to classify whether some code is or is not likely to exhibit legacy
problems.

Those heuristics match well with code written in a purely functional
style, which is what the second half of the chapter is concerned with.
The purely functional programming (pure FP) paradigm is introduced,
together with some characteristics and advantages of said paradigm.

The Method chapter begins by connecting the dots of legacy code and
pure FP. The purely functional language PureScript (PS) is introduced,
and some of the important characteristics of pure FP are explored in
PS, together with their advantages as concerning the earlier defined
heuristics.

Next, the process of the thesis project is presented. Biodalliance is
introduced as a legacy system, and the Graph Genetics Browser as an
application extending said system, as well as how this project
fulfilled the thesis objective and provided the data necessary to
evaluate the hypothesis.

The following Results chapter first presents the resulting browser,
then goes into some detail on the implementation of various parts
of the browser. These subsections serve to provide solid examples
of how pure FP can solve problems associated with legacy code, and
produce code that is less likely to exhibit similar problems in the
future.

In the Discussion, it is found that GGB and its codebase meets the
requirements. Then we go into more detail examining the suitability
of PS for working with legacy JS, and which characteristics of BD
made it more or less suitable for this process. Next a closer
examination is made of what parts of pure FP helped with legacy
problems, and the chapter ends with a walk through of some of the
developmental difficulties encountered during the project.

The report concludes, having found that pure FP was a good tool
especially for GGB and BD. Some other ways of gaining the advantages
of pure FP, without using PS or another pure FP language, are given.
This is followed by the limitations of the project, especially its
nature as a case study, and some ideas for future studies. Lastly
the future of GGB is presented.

With the hypothesis --- pure FP as a viable tool for working with
legacy code --- and an overview of the project in hand, the next
chapter dives into the concepts of legacy code and pure FP.


* Theory
The prerequisite theory for the thesis project is introduced. A
definition of "legacy code" is given, followed by relevant statistics,
facts, and techniques concerning legacy code and working with it. This
is followed by an introduction to the purely functional programming
paradigm.


** Legacy code
This section begins with defining legacy code, and presenting
statistics concerning its prevalence and costs. Why it is a problem,
some attempted solutions, and the causes in code of those problems,
follow. The section ends with defining code quality with respect to
legacy code.


*** Definition

There is no formal definition of "legacy code", but
\citeauthor{feathers2004working} gives the definition "Legacy code is
code that we've gotten from someone else". \citeauthor{Bennett1995}
provides a definition of legacy systems in general, which gives an
idea of why it may be problematic: "large software systems that we
don't know how to cope with but that are vital to our organization"
\cite{Bennett1995}. Finally, \citeauthor{weide1995reverse} gives a
definition closer to the spirit of the concept as experienced by
programmers working with legacy systems, in the trenches, as it were:

#+BEGIN_QUOTE
[..] legacy code, i.e., programs in which too much has been invested
just to throw away but which have proved to be obscure, mysterious,
and brittle in the face of maintenance.

\cite{weide1995reverse}
#+END_QUOTE

In other words, legacy code is code that continues to be relevant, e.g. by
providing a service that is important, and that requires modification,
or will require modification. If there were never going to be any reason to
modify the code, it would not be worth talking about, nor is it likely that
a system that provides a continually valuable service will not at some point
in the future require maintenance or new features \cite{lehman1980programs}.

For this reason, legacy systems are prevalent in the world.
If a system works as it should, providing the service that is needed,
and said service must continue to be provided, the safest thing to do
is to leave it as is --- until it is decided, for whatever reason, that
changes must be made.

The U.S. goverment federal IT budget for 2017 was over $89 billion, with
nearly 60% budgeted for operations and maintenance of agency systems,
with a study by the U.S. Government Accountability Office finding that
some federal agencies use systems that are up to 50 years old \cite{gao2016legacy}.

Many of these federal agency systems consist of old hardware, old operating
systems, etc., however the problems of a legacy system do not need to be
caused by such factors. The code itself is often the problem \cite{Bisbal1999},
and is what this thesis is concerned with.

We define a "legacy codebase" as the codebase of a legacy system,
where the problem of modifying the system is constrained by the code
itself --- the underlying technology is not relevant. Likewise we do
not look at dependencies, a problem solved by pure functional package
managers such as Nix \cite{dolstra2004nix} and Guix
\cite{courtes2013functional}.

Why would changes need to be made to a legacy codebase? When the
behavior of the system needs to be changed. \citeauthor{feathers2004working}
identifies four general reasons:

  1. Adding a feature
  2. Fixing a bug
  3. Improving the design
  4. Optimizing resource usage

All of these somehow modify the behavior of the system; if there was
no change in the system behavior, the change in code must have been
to some part of the codebase that is not used! Thus, the desired change
requires a change in behavior. The problem with legacy code is that
it is difficult to know how to make the change to the code that produces
this desired change in behavior, and /only/ the desired change.

The main reason it is difficult to work with legacy code is lack of
knowledge of the system and codebase, and how the behavior of the system
relates to the underlying code. Legacy codebases often lack
documentation and tests, without which a new programmer on the project
has few, if any, tools at their disposal to understand the codebase as
it is, since they do not have any knowledge of how and why the code
came to be as it is. Even if a design or system specification exists,
it is not necessarily accurate. The code may very well have grown
beyond the initial specification, and the specification need not have
been updated in step with the code.

For these reasons, one of the main problems of working with legacy
code is understanding it in the first place \cite{feathers2004working}
\cite{Bennett1995} \cite{siebra2016anticipated}. This is also a
difficult, time-consuming process, and one of the reasons reverse
engineering legacy systems is rarely, if ever, a cost-effective
undertaking \cite{weide1995reverse}. Also according to \citeauthor*{weide1995reverse},
even if a system is successfully reverse engineered and modified,
even if a /new/ system is successfully developed that provides
the same behavior as the legacy system but with a better design,
it is highly likely that the new system, eventually, reaches
a point where it too must be reverse engineered --- i.e. when the
"new" system becomes another legacy system, and the cycle begins anew.

In short, the problem with legacy code is lack of knowledge in what
the system does, and how the code relates to the system and its parts.
This makes it difficult to know what changes to make to the code to
produce the desired change in system behavior, and if a change made is safe,
i.e. that /no/ undesired change in system behavior results.


*** Solutions

Legacy code has been a recognized problem for decades, and people have
been trying to solve it for as long
\cite{lehman1980programs}
\cite{weide1995reverse}
\cite{Mntyl2006}.
This section will present some general approaches and tools that have
been applied when working with legacy systems. First, techniques that
help with manual reverse-engineering, which is the most widely used
approach when working with legacy code, are introduced, followed by a
brief walk through some automated tools.

**** Reverse engineering legacy systems

Reverse engineering a system can be very difficult and time-consuming,
even with access to the source code \cite{weide1995reverse}. There are
entire books dedicated to the subject of understanding a legacy codebase
and working with it to transform it into something more easily understood
and extended \cite{feathers2004working}.

As previously mentioned, the
greatest problems stem from insufficient knowledge of the system; from
not knowing what code does, or what happens when a change is made.
Thus, adding tests is one of the main tools for increasing understanding,
as tests provide the programmer with feedback on their code changes
\cite{siebra2016anticipated}.

However, this feedback is only as good as the test suite; if a
system behavior is not covered by the tests, the programmer has a blind
spot in that area.

Where and how to add tests is an art and science of its own, as the
programmer must find what parts of the system need to be tested, and
how to insert the tests into the codebase. This becomes more difficult
when a program has many tightly-knit parts, global state, etc.

With a robust test suite, a programmer can modify the codebase to improve the code quality
and architecture of the system, confident that their changes do not
compromise the system's behavior.


**** Automated approaches

Automated tools that assist with legacy systems are largely concerned
with increasing the knowledge of the system, understanding how it works,
extracting modules, etc. One example
is extracting OOP abstractions such as classes from imperative code,
e.g. by analyzing which functions and variables are used together
\cite{Etzkorn1997}
\cite{Silva2017}.

Another interesting route is creating a "modularization proposal," i.e.
a series of architectural changes to the codebase that lead to a
more modular system while minimizing change each step,
by constructing
a concept lattice based on where different global variables are used.
This lattice can then be used to create descriptions on how to
modularize the codebase \cite{lindig1997assessing}.

Another approach is using automated tools to detect potentially
problematic parts of a codebase, such as overly complex controllers
in GUI applications \cite{Lelli2016},
or code that is simply more likely to be buggy, based on statistical
analyses \cite{Ray2016}. These would help programmers find what parts
of the codebase to target.

Some ways that developers and researchers have attempted to fix
legacy codebases have been introduced, however one question
still remains: what is it with legacy code that makes it
problematic? Is there some attribute of the code itself, or
is /all/ code that successfully solves a problem doomed to
become "legacy" code? I.e., is it possible to write code that
is "legacy"-resistent?

We have identified that the problems seem to be related to
understanding the code. From that viewpoint, the question becomes: is
there a way to write code that is more easily understandible, and if
so, what characterizes it? This is what the next section seeks to
answer.

*** Code quality

The problems of legacy code revolve around knowing what different
parts of the codebase do; what changes in code lead to what changes
in behavior, and what changes in code do not lead to changes in
a subset of the system behavior. "Good" code could, then, be defined
as code which:

1. Tells the programmer what it does
2. Tells the programmer what it does *not* do

Conversely, "bad" code makes it difficult to see why it exists, why it
is called, and what effects it has, and does not have, on the system
behavior. These definitions beg further questions, however, and
require deeper investigation.

*Knowing what a piece of code does*
     means knowing what data it requires to do whatever it is it does,
i.e. what other code it depends on, and knowing what effects it has
on the system behavior.
This may sound simple, but a function or method
can easily grow to the point where it is difficult to see what the
dependencies truly are, e.g. if a compound data type is provided
as the input to a function, is every piece used? If so, how are those
pieces created; what parts of the codebase are truly responsible for the
input to the function? Knowing what code calls the function is not
enough unless the call site is entirely responsible for the input.

While dependencies may be difficult to unravel, the opposite problem
is often much worse.
Knowing what effects a piece of code has on the system can be extremely
difficult in commonly used languages such as Java and Python,
as there is often little limiting what some function or method can
do. A given method may perform a simple calculation on its input
and return the results. It may also perform an HTTP request to some
service and receive the results that way -- with no indication that
the system communicates with the outside world in this manner, nor
that the system /depends/ on said service. It could also modify global
state, which potentially changes the behavior of all other parts of
the codebase that interact with said state, despite there being no
direct interaction between the different parts. This has been compared
to the intuitively strange interactions of entangled particles in
quantum mechanics:

#+BEGIN_QUOTE
Most large software systems, even if apparently well-engineered on
a component-by-component basis, have proved to be incoherent as a
whole due to unanticipated long-range "weird interactions" among
supposedly independent parts.

\cite{weide1995reverse}
#+END_QUOTE

*Knowing what a piece of code does not do*
     is, for the above reasons, difficult in many languages. In fact,
while these two questions may appear very similar, it is often not the
case that knowing the answer to one of them lets the programmer deduce
the answer to the other.


*Good code*
     must then, somehow, communicate as much information to the programmer
as possible. However, throwing too much information at the programmer
will not help.

We have seen that legacy code is often if not always imperative code,
and that object-oriented programming (OOP) has often been tried as a
solution for legacy code. Despite this, many current legacy code
systems are written in OOP languages such as Java, and while books
provide dozens of ways to write good OOP code, and ways to reduce the
problems of legacy code, there does not seem to be any reason to
believe that OOP is the ultimate solution to legacy code, nor that OOP
code is inherently the best type of code.

Object-oriented is not the only way to write code. Functional
programming (FP) is a programming paradigm that takes quite a
different approach than OOP, and purely functional programming
provides tools to assist the developer in writing what we have now
defined to be "good" code. The next section provides an introduction
to the area and its ideas.


*** Summary

Legacy systems are an ubiquitous problem, costing enormous amounts of
money in maintenance and development costs. The definition can be
summed up as a system which is difficult to understand, and some
heuristics have been defined for identifying code that is or is not
likely to be difficult to understand. "Good" code is taken to be code
that is easy for a programmer to identify what it does with respect to
the system behavior, and what it does not do; "bad," then, is the
obvious antonym.

This definition of good code coincides with many of the characteristics
of code written in a purely functional style, which the next section
is concerned with.



** Purely functional programming


This section introduces the purely functional programming (pure FP) paradigm,
and its strengths. In short, pure FP enforces "referential transparency,"
which means that any piece of code can be replaced with the value it evaluates
to, wherever it appears in the source code. This makes it easier to understand
the code, any function can be reduced, on a cognitive level, to a black box
that can be passed around and used without having to think about its contents
\cite{hughes1989functional}.

Pure FP achieves this by using immutable data structures, eliminating side effects
in code, and using a powerful type system to express effectful computations
(e.g. code that interacts with the user).


*** Functional programming
The functional paradigm can be seen as a natural extension to the lambda
calculus, a model of computation invented by Alonzo Church, where a
small set of variable binding and substitution rules are used to express
computation.

The Turing machine and lambda calculus models of computation are
equivalent \cite{turing_1937}. However, while the Turing machine model
is largely an abstract concept that is useful for modeling
computation, but less so in actually solving programming problems,
there are several actively used languages that are, at their core,
some type of lambda calculus. One example is the purely functional
language Haskell. The Glasgow Haskell Compiler, the de facto standard
Haskell implementation, compiles to a language based on
System-F\omega, a typed lambda calculus, as an intermediate language
\cite{haskell2010} \cite{ghcguide}.


The main tool in the lambda calculus is defining and applying functions;
unsurprisingly, functions are the focus of FP. In this case, "function"
is defined in the mathematical sense, as a mapping from inputs to outputs,
rather than the sense of a function in e.g. the C programming language,
where it is rather a series of commands for the computer to execute.

In FP, if any function $f$ is given some input $x$ and produces some
output $y$, it must /always/ be the case that $f(x) = y$. Thus, wherever
$f(x)$ (for this given $x$) appears in the code, it can be replaced with $y$,
without changing the program behavior whatsoever. Conversely, if we have
that $g(a) = b$, but calling $g(a)$ prints a value to a console window,
$g$ is /not/ a function in this sense, as replacing $g(a)$ with $b$ in
the code would change the program behavior by not printing to the console.

*** Purity
This characteristic, that a function always produces the same output
given some input, including effects, is what defines a "pure"
function. Pure functions are referentially transparent, and can thus
be seen as black boxes. Conversely, an impure function, i.e. a
function with side-effects, cannot be referentially transparent.

An "effect" does not have to be something like interacting with the
user, or making an HTTP request. It also includes in-place mutation of data,
querying the OS for a random number, throwing an exception --- the list
goes on. In a pure FP language such as Haskell, these effects are
encoded in the language's type system, making it possible to write
programs in fact actually /do/ things, while enjoying the advantages
that pure FP provides.


*** Advantages
The advantages of pure FP are largely concerned with having the compiler
enforce program correctness. By encoding effects in types that are checked
by the compiler, the programmer is prevented from writing code that has
side-effects.

More generally, while writing a program in a pure functional language,
the programmer is encouraged by the language and environment to write
code that is reusable and easy to reason about \cite{hughes1989functional}.

Using a language with a powerful type system, it is also possible to
write code in such a way that the semantics of the program is, to some
extent, expressed in the types. This allows the compiler to enforce
/semantic/ program correctness. The programmer can construct
transformations between data structures and compose them to produce
a program, and said program is type-checked to ensure some degree
of correctness.

Another boon bestowed by a powerful type system such as Haskell's, is
that if a programmer can express their problem in the language of
category theory, they gain access to 70 years of documentation
concerning their problem. This may appear unlikely, but in fact
category theory appears to provide tools to naturally express many
problems encountered in software development. One example is
constructing and combining 2D vector diagrams \cite{yorgey2012monoids};
another, defining UIs in a declarative
manner \cite{freeman2017Comonads}.

If the abstractions used can be expressed in
the type system, the compiler can help prove that the program is
correct.


** Summary

Legacy code is largely a problem of understandable code, which in turn
is closely related to the effects a piece of code performs when run.

This introduction of pure FP has shown that one of the main features
is to eliminate side-effects of functions. A pure function must, by
definition, tell the compiler --- and the programmer --- what it does.
As important, a pure function cannot do anything else. Pure FP when
combined with a powerful type system such as that of Haskell has
many more benefits, providing the programmer with tools to express
the semantics of the program in such a way that the compiler can
ensure some level of correctness.

It does not seem like a large logical leap to expect these features of
pure FP to provide assistance when working with, or preventing, the
problems of legacy code. The next chapter goes into detail how this
hypothesis was tested.


* Method


The purpose of this thesis was to evaluate whether pure FP can be a
tool for working with legacy codebases, and if pure FP tends to lead
to code that is less likely to exhibit legacy problems.

This chapter begins with arguments in favor of the hypothesis,
enumerating some features of pure FP that appear advantageous for
limiting the potential problems associated with legacy code. Next, the
programming project that is the core of the thesis, the Graph Genetics
Browser project is introduced, followed by how the hypothesis was
evaluated in the context of this project.


** Functional programming and legacy code

As was shown in the previous chapter, pure FP has many tools and
features that are likely to limit the problems of legacy code.
This is not the first time this has been considered, as can be
seen in the following quote by Joe Armstrong, one of the creators
of the Erlang language:

#+BEGIN_QUOTE
I think the lack of reusability comes in object-oriented languages,
not functional languages. Because the problem with object-oriented
languages is they’ve got all this implicit environment that they carry
around with them. You wanted a banana but what you got was a gorilla
holding the banana and the entire jungle.

If you have referentially transparent code, if you have pure functions
--- all the data comes in its input arguments and everything goes out
and leave no state behind --- it’s incredibly reusable.

--- Joe Armstrong \cite{seibel2009coders}
#+END_QUOTE

This section begins by introducing PureScript, the programming
language used in the thesis project. It continues with providing
examples of language features that are likely to reduce the legacy
code-related problems and code patterns in general.

*** PureScript
PureScript is a purely functional programming language in the
style of Haskell, that compiles to JavaScript[fn:purescript]. PS
is immutable by default, pure, and has an advanced static type system
that gives the programmer many tools to increase productivity and
program correctness. Each of these features will be examined further
in the following sections.



[fn:purescript] http://www.purescript.org




*** Immutability
Some data being "immutable" means it does not change. PS being "immutable
by default" means that all values that one work with when writing PS code
are immutable --- nothing can change[fn:immutable-caveat]. Instead, if
a function e.g. sorts a list, it creates a new sorted list, rather than
modify the existing list. This ensures that all other parts of the program
that uses the input list continue to function as they did before the sorting
function was called. This would not have to be the case if the list changed
in memory. The programmer never has to think about copying values;
PS takes care of it.

Immutability by default reduces the reach of code by eliminating one
prominent side-effect. It provides the programmer with absolute
certainty that:

1. Inputs to a function are unchanged in that function
2. Calling a function on some value does not change that value


Mutation is one common side-effect in imperative programming, but far
from the only one. The next section considers effectful programming
in general.


[fn:immutable-caveat] There are some mutable data structures in PS, e.g.
arrays that support in-place mutation for efficiency. These are separate
from the immutable versions; transforming between the two must be done
explicitly. As mutation is an effect, so it too is captured by the type system.

*** Purity

A purely functional programming language does not only prevent the
side-effect of mutating data in a function; it prohibits functions
from causing /any/ side-effect[fn:side-effect-caveat]. Some examples
of effects follow.


[fn:side-effect-caveat] In PureScript, functions can be made impure
by improper use of the FFI, or by using "unsafe" functions such
as \texttt{unsafeCoerce}. This is not encouraged.


A /partial/ function is one that is not defined on all its inputs,
e.g. a function that tries to access an out of bounds index on
an array, whose type is given in listing  [[code:method-unsafe-index]].
If the given index is outside the array, the function explodes,
for example with an exception. Hardly what the type says it will
do, so \verb|unsafeIndex| is not truly a function.

#+name: code:method-unsafe-index
#+caption: Type of unsafe array indexing function
#+BEGIN_SRC purescript
unsafeIndex :: forall a. Array a -> Int -> a
#+END_SRC

Another effect is working with implicit state; another, retrieving
data from an external source, or updating the user interface. All of
these effects can be useful, possibly vital, for our programs, so it
would not be desirable to discard them in the name of purity. PS
provides tools to encode effects in the types of functions, which
enables us to write pure yet effectful functions. The next section
gives more details.


*** Static types

PS has a powerful type system that makes it possible to describe much
more of the semantics of the program in a way that the compiler can
describe and check for us.

For example, the effect of partiality can be captured in the type
system with the \verb|Maybe| type, defined in listing [[code:method-maybe]].
A value of type \verb|Maybe Int| can contain either some \verb|Int| wrapped in a
\verb|Just|, or \verb|Nothing|. When writing functions that take a \verb|Maybe| as
input, the PS compiler will ensure that both possibilities are
accounted for by failing with an error if something is missing.

#+name: code:method-maybe
#+caption: The Maybe data type
#+BEGIN_SRC purescript
data Maybe a
  = Just a
  | Nothing
#+END_SRC

With \verb|Maybe| it is possible to write a safe, pure version of \verb|index|,
see listing [[code:method-safe-index]]. The effect of potential failure
is captured in the function returning \verb|Maybe a| rather than \verb|a|.

#+name: code:method-safe-index
#+caption: Type for safe array indexing function
#+BEGIN_SRC purescript
index :: forall a. Array a -> Int -> Maybe a
#+END_SRC

In PS, another type is used to encapsulate so-called native effects,
such as printing to the console, updating the UI, etc. This is the

#+name: code:method-functor
#+caption: Functor typeclass definition
#+BEGIN_SRC purescript
class Functor f where
  map :: forall a b. (a -> b) -> f a -> f b
#+END_SRC

Often typeclasses are used to work with algebraic and
category-theoretic abstractions, which provide a powerful way to write
code that is both general and can be used to write code such that the
compiler can check the semantics of our program in a way that is
relevant to the actual program behavior.

Parametric polymorphism is reminiscent of generics in languages such
as Java, but more powerful. Consider again the function in listing
[[code:method-safe-index]]. The type says that it works
on \verb|forall a. Array a|; meaning it accepts any array, no matter what it contains.
This is what it means for a function to be "parametrically
polymorphic". Besides reducing code duplication by not having to write
one indexing function for \verb|Array Int|, another for \verb|Array Boolean|
etc., parametric polymorphism also provides some additional knowledge
about the function, by enforcing that the function cannot do anything
with some of its arguments other than pass them around. By looking at
the type signature for \verb|index|, we /know/ that the output (if it is
not \verb|Nothing|) must come from the given array, as the function cannot
create values of type \verb|a| from thin air.

A more extreme example is given in listing [[code:method-identity-fun]].
This is the type of the identity function; the identity function is in
fact the only function with this type. This is because it is
parametrically polymorphic --- the only thing \verb|id| can do with
its argument is return it, there is literally nothing else that can be
done.

#+name: code:method-identity-fun
#+caption: The identity function
#+BEGIN_SRC purescript
id :: forall a. a -> a
#+END_SRC

There is much more to PS' type system, but this covers most of what is
used in the thesis project.


*** Summary

PureScript is an excellent example of a purely functional language,
providing all of the features examined in the Theory chapter. As it
compiles to JS and has good support for interoperating with JS, it
is a natural candidate for investigating the viability of pure FP
as applied to a legacy codebase written in JS. The next section
presents the chosen legacy system, and how it will be extended.


** Extending a legacy system

In this section, Biodalliance, the legacy system that was extended, is
described. The extent and nature of the changes, and more details of
the resulting application, are also given.

*** Biodalliance

Biodalliance (screenshot in figure [[fig:method-bd-screenshot]]) is an
open source (BSD licensed) HTML5-based genome browser written in
JavaScript \cite{down2011dalliance}. It is fast, supports several data
formats commonly used in bioinformatics, and the plots displayed can
be configured and customized. Since it is HTML5-based, it can be
embedded into any web page and does not require any special tools to
be used. It also supports exporting images as SVG, which can be used
as high quality figures in publications.

#+caption: Screenshot of Biodalliance, showing genes and multiple tracks with various phenotypic data.
#+name: fig:method-bd-screenshot
[[./figures/bd2.png]]

For GN2 we want a genome browser that supports these features. We also
want to be able to add new features, however BD has shown itself to be
difficult to work with and extend, for reasons earlier defined as
legacy code problems[fn:gsoc2016-footnote]. BD has been in development
since 2010, and as of December 2017 consists of 17.5k lines of JS code
(sans comments and whitespace) divided over some 61
files[fn:bd-github]. The codebase has grown organically over time, and
has become complex, with single pieces of functionality (such as
rendering data to the HTML5 canvas) being split into several large
functions in different files, and difficult-to-grasp program control flow.


[fn:gsoc2016-footnote] The author previously worked on extending BD as
part of Google Summer of Code 2016, and encountered some difficulties:
https://chfi.se/posts/2016-08-22-gsoc-final.html


[fn:bd-github] Biodalliance's source code can be found on GitHub at \newline
https://github.com/dasmoth/dalliance


We want many of the features of BD, such as its support for many file
formats, which have required much time and effort to be implemented.
However, we also want new features, but the time and effort required
to implement them in BD is much greater than necessary due to the
legacy aspect of BD's codebase.

The solution that was decided upon was to develop a new genome
browser. This browser would allow embedding BD within it, and so be
compatible with BD and support the desired features. However, it would
be a separate application, and be independent of BD's baggage. This
application is the Genetics Graph Browser, to which the next section
is dedicated.


*** Genetics Graph Browser

The Genetics Graph Browser was written in PS. It begun as a tool for
constructing BD-compatible data structures for creating new ways to
plot data, but soon grew to a web application that embeds BD and
Cy.js, and provides communication between the two.

Throughout its development, various legacy-type problems were
encountered in BD, and avoided or solved in GGB. As GGB is written in
PS, this was done using various "features" of FP, from the sections above. In
this way, the GGB project can be seen as a case study in working with
legacy code using purely functional programming.

The main features of GGB include:
+ Working with JS APIs
+ Configuration
+ Units and types
+ Rendering data to the screen
+ Communication between BD and Cy.js
+ Creating a purely functional UI containing a legacy JS app

Besides wrapping BD and providing genome browser functionality, one of
the goals of GGB is to support exploratory data analysis of both
genome-based data as well as graph-based, semantic web data. Where BD
provides the genome browser, Cy.js (screenshot in figure
[[fig:method-cy-screenshot]]), a graph theory tool written in JS
\cite{franz2015cytoscape}, is wrapped to provide the graph-network
functionality.

#+caption: Screenshot of Cytoscape.js, displaying [[http://js.cytoscape.org/demos/colajs-graph/]].
#+name: fig:method-cy-screenshot
[[./figures/cy1.png]]

GGB supports connecting BD and Cy.js data, letting the user configure
interactions between the two browsers based on user interaction (e.g.
updating the Cy.js graph upon clicking on a gene in BD).

The hypothesis of this thesis was then evaluated by examining if and
how pure FP helped with these "legacy problems," both when fixing
problems or improving how a problem was solved in BD, including the
parts of GGB that interact with BD, as well as parts of GGB that are
not related to BD, but solutions to which would likely end up
exhibiting legacy-style problems.

PS was chosen as the language for the project for its support for
interoperation with JS --- PS can simply call JS functions and vice
versa. However, PS also enforces a purely functional programming
style, making it ideal for evaluating the hypothesis.


** Summary

The hypothesis, that functional programming techniques can help when
working with an existing legacy code base, as well as lead to code
that is less likely to exhibit legacy code problems in the future,
was tested by developing an application in PureScript that both
interfaces with an existing legacy genome browser, and is intended
to be a stand-alone browser in the future.

Identifying features whose implementation were already cause for
concern in BD, or had the potential to be implemented in problematic
ways in GGB, and if and how FP helped reduce or eliminate those
problems, provides a lens through which it was possible to make some
judgements as to the validity of the hypothesis.


* Results

In this chapter, the resulting product, that is, GGB, is presented.
After a brief overview of the browser itself, the rest of the chapter
consists of deeper dives into the parts of the source code that are
most relevant to the thesis objective.

** The Graph Genetics Browser

GGB, at the end of the thesis project, can be configured to display
genome-based data tracks in an embedded BD browser, and show
graph-based data in an embedded Cy.js browser. Configuration of these
browsers is done by providing a single configuration object to GGB,
which GGB verifies and uses to instantiate the embedded browsers.
There is also basic support for configurable interactions between the
browsers, making it possible for each browser to react to events in
the other. Finally, the insides of GGB has modules for easily creating
new ways to display data in BD, and works with genome-based data in
a unit-conscious manner.


The source code for GGB can be found in its GitHub repository at
\newline https://github.com/chfi/purescript-genetics-browser


Visually, GGB does not yet add anything substantial on its own; a
screenshot is elided as simply placing figure [[fig:method-bd-screenshot]]
just above figure [[fig:method-cy-screenshot]] provides a sufficient idea.

\pagebreak

** Translating JavaScript interfaces to PureScript

#+INCLUDE: "./bdcy.org"

\pagebreak

** Safe application configuration
# or: transforming user configuration to safe application options

#+INCLUDE: "./config.org"

\pagebreak

** Types and units
# or: domain-based type checking of transformations
#     (or something like that)

#+INCLUDE: "./units.org"

\pagebreak

** Rendering and positioning data on the screen

#+INCLUDE: "./glyph.org"

\pagebreak

** Transforming and handling events

#+INCLUDE: "./events.org"

\pagebreak

** User interface

#+INCLUDE: "./ui.org"

\pagebreak

** Summary of results

Many problems related to legacy code were encountered during
development, on various levels, from nitty-gritty line-by-line code to
large modules such as configuration; others were potential problems,
likely to rear their head when implementing software in domains like
that of GGB. The problems were dealt with in various ways using
different features of the language and libraries provided. The
resulting application is written in a purely functional language, but
also both embeds a legacy system within it, as well as providing tools
for extending said legacy system. Last, but certainly not least, it is
an extensible and flexible foundation for a new genome browser.

In the discussion that follows, some more general ways that pure FP
had an impact on this project are considered, and an effort is made to
extrapolate these results to other legacy systems.




* Discussion

The resulting browser, GGB, fills the specified requirements, and the
codebase does as well. PureScript was not a magic bullet, and some
problems were experienced during development, however the source code
on the whole does not, arguably, suffer from the problems of legacy
code as defined in this report.


The code from the previous chapter is examined to see if and how it
stuck to the ideas of "good" code, including what FP features
contributed. Problems experienced during development are presented,
and some examples of how GGB will be extended in the future are given,
together with estimates on how "easy" the addition of those parts to
the codebase and system will be. The potential ways to extend the
browser are also looked at from the view of BD, to examine how
difficult it would be to extend BD without the support of GGB.


** PureScript and legacy JavaScript code

PureScript was an excellent tool for developing GGB; in the author's
opinion, it was no doubt the best choice. PS's FFI capabilities made
it easy to wrap the native JS APIs of BD and Cy.js, then construct
type-safe PS APIs. This minimizes the reach of JS code into the GGB
codebase. Several times during development, when some part of the
system started behaving strangely, it was due to the FFI almost every
time. Even JS code, which due to its untyped dynamic nature
tends to lead to bugs in unexpected places, becomes easy to debug when
restricted to the very edges of the program.


However, it is possible that BD was uncharacteristically simple to
hook into with another application. While the JS API that is exposed
by BD is minimalistic, the "incisions" that had to be made into BD's
source code to make it possible for GGB to add some given functionality,
were few and quite small.


As an example, adding support for external renderers (something
done before this project as a part of Google Summer of Code 2016[fn:gsoc2016-renderers]),
while a large undertaking in terms of the time required to understand
what had to be done, and rewriting functions to better understand
the rendering system as a whole, very few changes were made to the
control flow of the rendering system. That is, few, if any, changes
that may have led to undesired changes of system behavior, were required.


[fn:gsoc2016-renderers] For more details, see the author's blog post:
https://chfi.se/posts/2016-07-26-gsoc-render-complete.html


On the GGB side, this can be seen in the fact that the entire \verb|Glyph|
system of defining new ways of displaying data is mapped to BD by
one relatively small function, seen in listing [[code:glyph-together-1]].

\verb|Glyph| is also an excellent argument for PS and GGB being extensible
and easy to maintain. For example, if a more precise way of checking
whether a glyph covers a given point on the canvas (something currently
done by providing axis-aligned bounding boxes) is desired, it is simple
to create a new interpreter that maps glyphs to some other more exact
collision shape.


Another potential change that is easily facilitated by \verb|Glyph|'s
construction using the \verb|Free| monad would be to add support for new
ways of rendering the browser, e.g. using WebGL. Write another
interpreter that maps glyphs to whatever the rendering system
requires, and suddenly all existing glyphs (and, by extension, all
functions that e.g. work on collections of glyphs) are compatible with
this new rendering system.

In short, embedding a legacy JS app in a PS app was in this case
an efficient way of achieving the development goals of
the project, as interfacing with BD required little overhead.
It helped that work had already been done in that area; other
projects may or may not present difficulties for the approach
taken here.

** Functional and quality code

Recall that our definition of "good code" is code which is easy
to understand when looked at. Our heuristics for good code include
functions that do not have far-reaching or otherwise difficult to
predict side-effects; conversely, it is also good to be able to
see at a glance what the purpose of some given function is.

Let us first examine how pure FP has led to functions and code that
more clearly presents their purpose.

*Configuration* in GGB is done by parsing a foreign JS object into a
record; this record is then used to initialize the browser itself.

*Code reuse* is increased by features such as typeclasses and
parametric polymorphism. The use of a \verb|Free| monad DSL in the \verb|Glyph|
parts is a good example of this.

*Fewer logic problems* thanks to representing values in types that
correspond to their actual units as semantically relevant to the
program behavior, rather than their runtime representation.

*Abstractions* such as \verb|Semigroup|, \verb|Functor|, etc. provide the
programmer with information about what a function does. Listings
[[code:discussion-concrete-types]] and [[code:discussion-constraints]] show
the type signatures for two functions, \verb|mystery1| and \verb|mystery2|. The
function implementations are in fact the same, but the information
given in the types is not.

If we look only at \verb|mystery1|, the type give us some idea of what the
function does, but not much. Does it return the first list in the
array? The last? Maybe it creates a list containing the maximum value
of each list in the array. Because the function knows so much about
the types it works on, there is a lot that it can do.

On the other hand, see the type of \verb|mystery2|. The only tools at our
disposal are those from the \verb|Foldable| and \verb|Monoid| typeclasses: from
\verb|Foldable|, we know we can walk through the collection, applying a
binary function on the current and next value; from \verb|Monoid|, we have
a binary operation that combines two of the values in the collection,
and we can create an empty value. Code block [[code:discussion-fold]]
shows the implementation.

#+name: code:discussion-concrete-types
#+caption: A concrete type for some function on an array of lists
#+BEGIN_SRC purescript
mystery1 :: Array (List Int)
         -> List Int
#+END_SRC

#+name: code:discussion-constraints
#+caption: A general type for some function on some foldable of a monoid
#+BEGIN_SRC purescript
mystery2 :: forall f m.
            Foldable f
         => Monoid m
         -> f m
         -> m
#+END_SRC

#+name: code:discussion-fold
#+caption: The mystery is solved
#+BEGIN_SRC purescript
mystery2 = fold
mystery1 = mystery2
#+END_SRC

Next, the impact of pure FP on understanding the consequences of
calling functions. One of the reasons that it can be difficult to see
what the effects of calling some function in an impure language such
as JS is that the function can not only do more or less whatever it
wants with whatever it has at its disposal (which is not necessarily
easy to discover, either), but it can also call other functions with
arbitrary effects, and so on, and so on. The programmer must walk the
entire path of function calls to discover what happens.

Pure FP inverts this, as all effects, be it potential failure in a
function, or updating the browser DOM, or mutating a mutable array,
are encapsulated in the type system. This makes it impossible to call
an effectful function from a pure function. The upshot of this
inversion is that code cannot cause far-reaching consequences; there
is none of the "spooky action at a distance" that is one of the main
problems when trying to comprehend legacy code.

** Developmental difficulties

There was not much of a design specification for GGB at the start,
and as the author did not have previous experience with developing
a large pure FP application, there were many design and implementation
dead-ends on the way to the current state of the product, as it was
difficult to decide on the architecture of the application, and
sometimes how to approach and solve smaller problems. Deciding on
some abstraction too early was a mistake repeatedly made, leading
to refactoring to a more specific implementation[fn:author-apology].
Often, once the problem was solved, a more general solution showed
itself, and another refactoring was made. Thanks to the type system,
even large refactorings were quite easy, as the compiler largely
ensured that the code at the end of the refactoring effort behaved the
same as before.


[fn:author-apology] Admittedly, the author tended to attempt to use
newly learned (but not necessarily understood) concepts and abstractions
throughout the project.


Pure FP is a very different beast from imperative programming, not
only when it comes to larger-scale program design. Many of the
techniques used in this project require considerably more knowledge to
understand than similar solutions in imperative languages. Using
typeclass constraints (\verb|Semigroup| etc.) require the programmer to
know their meaning, which can become overwhelming when a function uses
half a dozen or more constraints[fn:abstraction-apology]. Highly
abstract code also tends to become small and terse, which can also be
a problem, especially for newcomers to the language and paradigm.


[fn:abstraction-apology] On the other hand, one only needs to learn how
\texttt{Semigroup} works once.


This is not to imply that these abstractions etc. must be used to write good
PS code, nor that they are required to gain any of the benefits of
pure FP. They can in fact be ignored, while still reaping most of
the benefits, as immutability, purity, and type-safety are still
provided.

However, it can be difficult to ignore, as one inevitably stumbles
upon them when reading library code --- something that is likely
to occur, as many libraries suffer from having too little documentation.
This is a symptom of PS being such a young language, rather than
some inherent problem with it. Fortunately this problem is mitigated
by PS having a very enthusiastic and welcoming community, filled
with people glad to help newcomers with their problems.


** Summary

Pure FP was in many ways a boon in the development of GGB. While there
were problems during development, their impact was limited by the
support of the type system and purity; nor is it likely that other
problems would not have occurred if GGB had been developed using in
imperative language and idioms.

Other legacy systems are likely to present their own difficulties if
one were to attempt a similar project with them, however the opposite
may also be true. In the next chapter, other tools that could provide
similar benefits are considered, along the extent to which PS was
responsible for the benefits in this case. Potential future areas of
research are also examined, as is the future of GGB.


* Conclusion

Pure FP is a good tool for GGB and BD, however other legacy systems
may be more difficult to use the techniques presented in this thesis.
For example, it may not be possible to find single points of entry a
lá the glyphs in BD's rendering system, or it may simply not be
feasible to embed the application due to its function, language, etc.

Even then, it is possible to write pure code, and take advantage of
its benefits. Pure functions can be written in practically any
language, and they provide the same benefit as shown in this report.
Abstractions such as provided by the \verb|Monoid| and \verb|Functor|
typeclasses can also be represented in other languages. Doing this
will lead to code that solves problems in ways much like code written
in PS.

Why use a language such as PS, then? What one misses out on by using
an impure language is the enforced purity and encapsulated effects;
that is, the compilator cannot help as much. Sometimes other tools can
be configured to enforce purity in an impure language, e.g. Bodil
Stokke's \verb|cleanjs| configuration for ESLint, a JS
linter[fn:cleanjs-link].


[fn:cleanjs-link] \texttt{cleanjs} can be found on GitHub at \newline
https://github.com/bodil/eslint-config-cleans


However, without a type system akin to that of PS, the compilator
simply cannot provide much assistance. That is, one gains the
advantages of code that is less likely to have far-reaching and
difficult to predict consequences, but it is still up to the
programmer to keep the program semantics in their head, and ensure
that the program is correct. Conversely, there are fewer tools to
assist the programmer in understanding the code and how it relates
to the system.


To summarize, pure FP has its own set of problems, however those
problems largely concern the knowledge of techniques, idioms, etc.,
rather than the knowledge of systems developed with it. These
can be seen as "up front" problems, which can be solved with developer
training. If it is the case that pure FP reduces legacy code problems,
this is a much smaller investment than maintaining a legacy system.


** Limitations and future work

This project was only that, a single project. It is impossible to draw
conclusions that are in any way definite with a sample size of 1;
hopefully larger, more quantitative studies are undertaken in the
future.

More generally, there has been little research performed on comparing programming
languages, and even less research on the user experience of languages,
i.e. on what actually makes a good
language \cite[p.~252]{seibel2009coders}. That is another avenue for
future research.

This is an area of research that it is difficult to do studies in, as
while there are plenty of legacy systems to go around, maintaining one
is expensive enough without experimenting with new approaches, or
spending additional money on the overhead of performing studies. When
one considers that to get enough data to learn something concrete,
experiments such as this one would have to be done on many legacy
systems, it seems unlikely that such research will be done.

Another way would be to analyze open source software projects. The
data already exists in the form of source control repositories, commit
logs, etc. There would be other difficulties, such as controlling for
e.g. community culture and other differences between languages used.
Nevertheless, some studies have been done in this area
recently \cite{Ray2014}, and the future may be promising.


** Future of the Graph Genetics Browser

GGB will continue to evolve beyond what has been described in this
report, including more tools for interactive exploration of
genome/graph data, and a genome browser written in PS which would
replicate some of the functionality of BD.

Based on the development process thus far, including the fact that
several large refactoring efforts have been successfully undertaken,
it appears likely that GGB's codebase is flexible enough that adding
these features will not be too difficult.

# Bibliography

# \nocite{*}
\bibliographystyle{plainnat}
\bibliography{bibliography}
